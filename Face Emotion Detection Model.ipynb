{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6280c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d190e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 512\n",
    "\n",
    "train_data_dir = './DataSet/train/'\n",
    "validation_data_dir = './DataSet/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d16bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 4394 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Let's use some data augmentaiton \n",
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "      rotation_range=10,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      )\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "#         color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "#         color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464ba421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgusted': 1, 'fearful': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprised': 6}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faab00ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 48, 48, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 46, 46, 8)    216         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 46, 46, 8)   32          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 46, 46, 8)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 44, 44, 8)    576         ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 44, 44, 8)   32          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 44, 44, 8)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 42, 42, 8)    576         ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 42, 42, 8)   32          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 42, 42, 8)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 42, 42, 16)  200         ['activation_2[0][0]']           \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 42, 42, 16)  64          ['separable_conv2d[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 42, 42, 16)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 42, 42, 16)  400         ['activation_3[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 42, 42, 16)  64          ['separable_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 42, 42, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 42, 42, 16)  400         ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 42, 42, 16)  64          ['separable_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 42, 42, 16)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 42, 42, 16)  400         ['activation_5[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 42, 42, 16)  64          ['separable_conv2d_3[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 21, 21, 16)   128         ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 21, 21, 16)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 21, 21, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 21, 21, 16)   0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 21, 21, 32)  656         ['add[0][0]']                    \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 21, 21, 32)  128         ['separable_conv2d_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 21, 21, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 21, 21, 32)  1312        ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 21, 21, 32)  128         ['separable_conv2d_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 21, 21, 32)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_6 (SeparableC  (None, 21, 21, 32)  1312        ['activation_7[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 21, 21, 32)  128         ['separable_conv2d_6[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 11, 11, 32)   512         ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 11, 11, 32)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 11, 11, 32)  128         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 11, 11, 32)   0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_7 (SeparableC  (None, 11, 11, 64)  2336        ['add_1[0][0]']                  \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 11, 11, 64)  256         ['separable_conv2d_7[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 11, 11, 64)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_8 (SeparableC  (None, 11, 11, 64)  4672        ['activation_8[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 11, 11, 64)  256         ['separable_conv2d_8[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 11, 11, 64)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_9 (SeparableC  (None, 11, 11, 64)  4672        ['activation_9[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 11, 11, 64)  256         ['separable_conv2d_9[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 6, 6, 64)     2048        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 64)    0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 6, 6, 64)    256         ['conv2d_5[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 6, 6, 64)     0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_10 (Separable  (None, 6, 6, 128)   8768        ['add_2[0][0]']                  \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 6, 6, 128)   512         ['separable_conv2d_10[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 6, 6, 128)    0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_11 (Separable  (None, 6, 6, 128)   17536       ['activation_10[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 6, 6, 128)   512         ['separable_conv2d_11[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 6, 6, 128)    0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_12 (Separable  (None, 6, 6, 128)   17536       ['activation_11[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 6, 6, 128)   512         ['separable_conv2d_12[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 3, 3, 128)    8192        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 128)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 3, 3, 128)   512         ['conv2d_6[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 3, 3, 128)    0           ['max_pooling2d_3[0][0]',        \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 3, 3, 7)      8071        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 7)           0           ['conv2d_7[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " predictions (Activation)       (None, 7)            0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 84,519\n",
      "Trainable params: 82,519\n",
      "Non-trainable params: 2,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def mini_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n",
    "    regularization = l2(l2_regularization)\n",
    "\n",
    "    # base\n",
    "    img_input = Input(input_shape)\n",
    "    x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
    "               use_bias=False)(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
    "               use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
    "               use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # module 1\n",
    "    residual = Conv2D(16, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    # module 2\n",
    "    residual = Conv2D(32, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    # module 3\n",
    "    residual = Conv2D(64, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    # module 4\n",
    "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = Conv2D(num_classes, (3, 3),\n",
    "               # kernel_regularizer=regularization,\n",
    "               padding='same')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Activation('softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model(img_input, output)\n",
    "    return model\n",
    "\n",
    "model = mini_XCEPTION((48, 48, 3), num_classes=7)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91d1e418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1ravi\\Anaconda\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.1093 - accuracy: 0.2184\n",
      "Epoch 1: saving model to ./emotion_detector_models\\model_1.hdf5\n",
      "56/56 [==============================] - 38s 577ms/step - loss: 2.1093 - accuracy: 0.2184 - val_loss: 2.1531 - val_accuracy: 0.1886\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.0180 - accuracy: 0.2475\n",
      "Epoch 2: saving model to ./emotion_detector_models\\model_2.hdf5\n",
      "56/56 [==============================] - 29s 504ms/step - loss: 2.0180 - accuracy: 0.2475 - val_loss: 2.1754 - val_accuracy: 0.1922\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9950 - accuracy: 0.2653\n",
      "Epoch 3: saving model to ./emotion_detector_models\\model_3.hdf5\n",
      "56/56 [==============================] - 29s 518ms/step - loss: 1.9950 - accuracy: 0.2653 - val_loss: 2.1913 - val_accuracy: 0.1925\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9773 - accuracy: 0.2724\n",
      "Epoch 4: saving model to ./emotion_detector_models\\model_4.hdf5\n",
      "56/56 [==============================] - 29s 511ms/step - loss: 1.9773 - accuracy: 0.2724 - val_loss: 2.2187 - val_accuracy: 0.1931\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9660 - accuracy: 0.2776\n",
      "Epoch 5: saving model to ./emotion_detector_models\\model_5.hdf5\n",
      "56/56 [==============================] - 30s 538ms/step - loss: 1.9660 - accuracy: 0.2776 - val_loss: 2.2163 - val_accuracy: 0.1908\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9571 - accuracy: 0.2850\n",
      "Epoch 6: saving model to ./emotion_detector_models\\model_6.hdf5\n",
      "56/56 [==============================] - 30s 523ms/step - loss: 1.9571 - accuracy: 0.2850 - val_loss: 2.1852 - val_accuracy: 0.1903\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9412 - accuracy: 0.2940\n",
      "Epoch 7: saving model to ./emotion_detector_models\\model_7.hdf5\n",
      "56/56 [==============================] - 30s 526ms/step - loss: 1.9412 - accuracy: 0.2940 - val_loss: 2.1361 - val_accuracy: 0.1752\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9300 - accuracy: 0.2993\n",
      "Epoch 8: saving model to ./emotion_detector_models\\model_8.hdf5\n",
      "56/56 [==============================] - 30s 527ms/step - loss: 1.9300 - accuracy: 0.2993 - val_loss: 2.0830 - val_accuracy: 0.1476\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9247 - accuracy: 0.3022\n",
      "Epoch 9: saving model to ./emotion_detector_models\\model_9.hdf5\n",
      "56/56 [==============================] - 30s 546ms/step - loss: 1.9247 - accuracy: 0.3022 - val_loss: 2.0290 - val_accuracy: 0.1610\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9118 - accuracy: 0.3109\n",
      "Epoch 10: saving model to ./emotion_detector_models\\model_10.hdf5\n",
      "56/56 [==============================] - 30s 530ms/step - loss: 1.9118 - accuracy: 0.3109 - val_loss: 1.9725 - val_accuracy: 0.2302\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9019 - accuracy: 0.3134\n",
      "Epoch 11: saving model to ./emotion_detector_models\\model_11.hdf5\n",
      "56/56 [==============================] - 31s 546ms/step - loss: 1.9019 - accuracy: 0.3134 - val_loss: 1.9509 - val_accuracy: 0.2667\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8932 - accuracy: 0.3200\n",
      "Epoch 12: saving model to ./emotion_detector_models\\model_12.hdf5\n",
      "56/56 [==============================] - 30s 550ms/step - loss: 1.8932 - accuracy: 0.3200 - val_loss: 1.9323 - val_accuracy: 0.2757\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8863 - accuracy: 0.3246\n",
      "Epoch 13: saving model to ./emotion_detector_models\\model_13.hdf5\n",
      "56/56 [==============================] - 31s 542ms/step - loss: 1.8863 - accuracy: 0.3246 - val_loss: 1.9682 - val_accuracy: 0.2528\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8727 - accuracy: 0.3280\n",
      "Epoch 14: saving model to ./emotion_detector_models\\model_14.hdf5\n",
      "56/56 [==============================] - 31s 543ms/step - loss: 1.8727 - accuracy: 0.3280 - val_loss: 2.0058 - val_accuracy: 0.2533\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8667 - accuracy: 0.3342\n",
      "Epoch 15: saving model to ./emotion_detector_models\\model_15.hdf5\n",
      "56/56 [==============================] - 32s 564ms/step - loss: 1.8667 - accuracy: 0.3342 - val_loss: 1.9631 - val_accuracy: 0.2723\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8553 - accuracy: 0.3382\n",
      "Epoch 16: saving model to ./emotion_detector_models\\model_16.hdf5\n",
      "56/56 [==============================] - 33s 578ms/step - loss: 1.8553 - accuracy: 0.3382 - val_loss: 1.9634 - val_accuracy: 0.2804\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8473 - accuracy: 0.3438\n",
      "Epoch 17: saving model to ./emotion_detector_models\\model_17.hdf5\n",
      "56/56 [==============================] - 32s 570ms/step - loss: 1.8473 - accuracy: 0.3438 - val_loss: 1.9393 - val_accuracy: 0.2838\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8376 - accuracy: 0.3483\n",
      "Epoch 18: saving model to ./emotion_detector_models\\model_18.hdf5\n",
      "56/56 [==============================] - 32s 558ms/step - loss: 1.8376 - accuracy: 0.3483 - val_loss: 1.9756 - val_accuracy: 0.2801\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8272 - accuracy: 0.3510\n",
      "Epoch 19: saving model to ./emotion_detector_models\\model_19.hdf5\n",
      "56/56 [==============================] - 34s 609ms/step - loss: 1.8272 - accuracy: 0.3510 - val_loss: 1.9574 - val_accuracy: 0.2879\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8205 - accuracy: 0.3560\n",
      "Epoch 20: saving model to ./emotion_detector_models\\model_20.hdf5\n",
      "56/56 [==============================] - 33s 574ms/step - loss: 1.8205 - accuracy: 0.3560 - val_loss: 1.9564 - val_accuracy: 0.2902\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8085 - accuracy: 0.3582\n",
      "Epoch 21: saving model to ./emotion_detector_models\\model_21.hdf5\n",
      "56/56 [==============================] - 32s 572ms/step - loss: 1.8085 - accuracy: 0.3582 - val_loss: 1.8922 - val_accuracy: 0.3270\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.8029 - accuracy: 0.3641\n",
      "Epoch 22: saving model to ./emotion_detector_models\\model_22.hdf5\n",
      "56/56 [==============================] - 32s 560ms/step - loss: 1.8029 - accuracy: 0.3641 - val_loss: 1.8885 - val_accuracy: 0.3139\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7911 - accuracy: 0.3674\n",
      "Epoch 23: saving model to ./emotion_detector_models\\model_23.hdf5\n",
      "56/56 [==============================] - 31s 552ms/step - loss: 1.7911 - accuracy: 0.3674 - val_loss: 1.9698 - val_accuracy: 0.2812\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7866 - accuracy: 0.3686\n",
      "Epoch 24: saving model to ./emotion_detector_models\\model_24.hdf5\n",
      "56/56 [==============================] - 34s 605ms/step - loss: 1.7866 - accuracy: 0.3686 - val_loss: 1.9083 - val_accuracy: 0.3083\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7766 - accuracy: 0.3738\n",
      "Epoch 25: saving model to ./emotion_detector_models\\model_25.hdf5\n",
      "56/56 [==============================] - 31s 556ms/step - loss: 1.7766 - accuracy: 0.3738 - val_loss: 1.8711 - val_accuracy: 0.3256\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7674 - accuracy: 0.3778\n",
      "Epoch 26: saving model to ./emotion_detector_models\\model_26.hdf5\n",
      "56/56 [==============================] - 32s 560ms/step - loss: 1.7674 - accuracy: 0.3778 - val_loss: 1.8839 - val_accuracy: 0.3195\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7610 - accuracy: 0.3798\n",
      "Epoch 27: saving model to ./emotion_detector_models\\model_27.hdf5\n",
      "56/56 [==============================] - 32s 558ms/step - loss: 1.7610 - accuracy: 0.3798 - val_loss: 1.9519 - val_accuracy: 0.2860\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7546 - accuracy: 0.3817\n",
      "Epoch 28: saving model to ./emotion_detector_models\\model_28.hdf5\n",
      "56/56 [==============================] - 32s 563ms/step - loss: 1.7546 - accuracy: 0.3817 - val_loss: 1.8341 - val_accuracy: 0.3354\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7452 - accuracy: 0.3840\n",
      "Epoch 29: saving model to ./emotion_detector_models\\model_29.hdf5\n",
      "56/56 [==============================] - 32s 560ms/step - loss: 1.7452 - accuracy: 0.3840 - val_loss: 1.8515 - val_accuracy: 0.3329\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7422 - accuracy: 0.3842\n",
      "Epoch 30: saving model to ./emotion_detector_models\\model_30.hdf5\n",
      "56/56 [==============================] - 33s 576ms/step - loss: 1.7422 - accuracy: 0.3842 - val_loss: 1.8316 - val_accuracy: 0.3298\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7305 - accuracy: 0.3931\n",
      "Epoch 31: saving model to ./emotion_detector_models\\model_31.hdf5\n",
      "56/56 [==============================] - 32s 568ms/step - loss: 1.7305 - accuracy: 0.3931 - val_loss: 1.8070 - val_accuracy: 0.3393\n",
      "Epoch 32/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7241 - accuracy: 0.3971\n",
      "Epoch 32: saving model to ./emotion_detector_models\\model_32.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.7241 - accuracy: 0.3971 - val_loss: 1.8316 - val_accuracy: 0.3368\n",
      "Epoch 33/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7160 - accuracy: 0.3966\n",
      "Epoch 33: saving model to ./emotion_detector_models\\model_33.hdf5\n",
      "56/56 [==============================] - 32s 560ms/step - loss: 1.7160 - accuracy: 0.3966 - val_loss: 1.8096 - val_accuracy: 0.3393\n",
      "Epoch 34/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7109 - accuracy: 0.3987\n",
      "Epoch 34: saving model to ./emotion_detector_models\\model_34.hdf5\n",
      "56/56 [==============================] - 32s 565ms/step - loss: 1.7109 - accuracy: 0.3987 - val_loss: 1.8300 - val_accuracy: 0.3479\n",
      "Epoch 35/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7076 - accuracy: 0.4018\n",
      "Epoch 35: saving model to ./emotion_detector_models\\model_35.hdf5\n",
      "56/56 [==============================] - 31s 548ms/step - loss: 1.7076 - accuracy: 0.4018 - val_loss: 1.8636 - val_accuracy: 0.3136\n",
      "Epoch 36/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.7027 - accuracy: 0.3978\n",
      "Epoch 36: saving model to ./emotion_detector_models\\model_36.hdf5\n",
      "56/56 [==============================] - 32s 559ms/step - loss: 1.7027 - accuracy: 0.3978 - val_loss: 1.8190 - val_accuracy: 0.3477\n",
      "Epoch 37/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6910 - accuracy: 0.4073\n",
      "Epoch 37: saving model to ./emotion_detector_models\\model_37.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.6910 - accuracy: 0.4073 - val_loss: 1.8569 - val_accuracy: 0.3262\n",
      "Epoch 38/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6785 - accuracy: 0.4084\n",
      "Epoch 38: saving model to ./emotion_detector_models\\model_38.hdf5\n",
      "56/56 [==============================] - 31s 546ms/step - loss: 1.6785 - accuracy: 0.4084 - val_loss: 1.7868 - val_accuracy: 0.3613\n",
      "Epoch 39/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6751 - accuracy: 0.4154\n",
      "Epoch 39: saving model to ./emotion_detector_models\\model_39.hdf5\n",
      "56/56 [==============================] - 31s 555ms/step - loss: 1.6751 - accuracy: 0.4154 - val_loss: 1.8232 - val_accuracy: 0.3334\n",
      "Epoch 40/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6668 - accuracy: 0.4165\n",
      "Epoch 40: saving model to ./emotion_detector_models\\model_40.hdf5\n",
      "56/56 [==============================] - 31s 544ms/step - loss: 1.6668 - accuracy: 0.4165 - val_loss: 1.7924 - val_accuracy: 0.3415\n",
      "Epoch 41/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6650 - accuracy: 0.4164\n",
      "Epoch 41: saving model to ./emotion_detector_models\\model_41.hdf5\n",
      "56/56 [==============================] - 31s 552ms/step - loss: 1.6650 - accuracy: 0.4164 - val_loss: 1.7774 - val_accuracy: 0.3527\n",
      "Epoch 42/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6566 - accuracy: 0.4202\n",
      "Epoch 42: saving model to ./emotion_detector_models\\model_42.hdf5\n",
      "56/56 [==============================] - 31s 552ms/step - loss: 1.6566 - accuracy: 0.4202 - val_loss: 1.7847 - val_accuracy: 0.3555\n",
      "Epoch 43/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6534 - accuracy: 0.4242\n",
      "Epoch 43: saving model to ./emotion_detector_models\\model_43.hdf5\n",
      "56/56 [==============================] - 33s 583ms/step - loss: 1.6534 - accuracy: 0.4242 - val_loss: 1.7506 - val_accuracy: 0.3680\n",
      "Epoch 44/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6464 - accuracy: 0.4224\n",
      "Epoch 44: saving model to ./emotion_detector_models\\model_44.hdf5\n",
      "56/56 [==============================] - 32s 560ms/step - loss: 1.6464 - accuracy: 0.4224 - val_loss: 1.7212 - val_accuracy: 0.3750\n",
      "Epoch 45/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6417 - accuracy: 0.4261\n",
      "Epoch 45: saving model to ./emotion_detector_models\\model_45.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.6417 - accuracy: 0.4261 - val_loss: 1.7633 - val_accuracy: 0.3610\n",
      "Epoch 46/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6354 - accuracy: 0.4251\n",
      "Epoch 46: saving model to ./emotion_detector_models\\model_46.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 1.6354 - accuracy: 0.4251 - val_loss: 1.7067 - val_accuracy: 0.3870\n",
      "Epoch 47/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6251 - accuracy: 0.4307\n",
      "Epoch 47: saving model to ./emotion_detector_models\\model_47.hdf5\n",
      "56/56 [==============================] - 31s 554ms/step - loss: 1.6251 - accuracy: 0.4307 - val_loss: 1.7096 - val_accuracy: 0.3817\n",
      "Epoch 48/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6223 - accuracy: 0.4267\n",
      "Epoch 48: saving model to ./emotion_detector_models\\model_48.hdf5\n",
      "56/56 [==============================] - 31s 553ms/step - loss: 1.6223 - accuracy: 0.4267 - val_loss: 1.6914 - val_accuracy: 0.3937\n",
      "Epoch 49/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6187 - accuracy: 0.4300\n",
      "Epoch 49: saving model to ./emotion_detector_models\\model_49.hdf5\n",
      "56/56 [==============================] - 32s 574ms/step - loss: 1.6187 - accuracy: 0.4300 - val_loss: 1.7415 - val_accuracy: 0.3619\n",
      "Epoch 50/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6116 - accuracy: 0.4332\n",
      "Epoch 50: saving model to ./emotion_detector_models\\model_50.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 1.6116 - accuracy: 0.4332 - val_loss: 1.6581 - val_accuracy: 0.4015\n",
      "Epoch 51/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.6062 - accuracy: 0.4364\n",
      "Epoch 51: saving model to ./emotion_detector_models\\model_51.hdf5\n",
      "56/56 [==============================] - 32s 558ms/step - loss: 1.6062 - accuracy: 0.4364 - val_loss: 1.7192 - val_accuracy: 0.3742\n",
      "Epoch 52/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5973 - accuracy: 0.4355\n",
      "Epoch 52: saving model to ./emotion_detector_models\\model_52.hdf5\n",
      "56/56 [==============================] - 31s 557ms/step - loss: 1.5973 - accuracy: 0.4355 - val_loss: 1.6762 - val_accuracy: 0.4021\n",
      "Epoch 53/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5963 - accuracy: 0.4410\n",
      "Epoch 53: saving model to ./emotion_detector_models\\model_53.hdf5\n",
      "56/56 [==============================] - 32s 563ms/step - loss: 1.5963 - accuracy: 0.4410 - val_loss: 1.6648 - val_accuracy: 0.4023\n",
      "Epoch 54/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5893 - accuracy: 0.4390\n",
      "Epoch 54: saving model to ./emotion_detector_models\\model_54.hdf5\n",
      "56/56 [==============================] - 32s 560ms/step - loss: 1.5893 - accuracy: 0.4390 - val_loss: 1.6857 - val_accuracy: 0.4015\n",
      "Epoch 55/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5827 - accuracy: 0.4455\n",
      "Epoch 55: saving model to ./emotion_detector_models\\model_55.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 1.5827 - accuracy: 0.4455 - val_loss: 1.6751 - val_accuracy: 0.3959\n",
      "Epoch 56/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5729 - accuracy: 0.4502\n",
      "Epoch 56: saving model to ./emotion_detector_models\\model_56.hdf5\n",
      "56/56 [==============================] - 32s 565ms/step - loss: 1.5729 - accuracy: 0.4502 - val_loss: 1.6289 - val_accuracy: 0.4213\n",
      "Epoch 57/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5714 - accuracy: 0.4485\n",
      "Epoch 57: saving model to ./emotion_detector_models\\model_57.hdf5\n",
      "56/56 [==============================] - 31s 554ms/step - loss: 1.5714 - accuracy: 0.4485 - val_loss: 1.6010 - val_accuracy: 0.4286\n",
      "Epoch 58/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5667 - accuracy: 0.4493\n",
      "Epoch 58: saving model to ./emotion_detector_models\\model_58.hdf5\n",
      "56/56 [==============================] - 31s 555ms/step - loss: 1.5667 - accuracy: 0.4493 - val_loss: 1.6694 - val_accuracy: 0.3929\n",
      "Epoch 59/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5583 - accuracy: 0.4536\n",
      "Epoch 59: saving model to ./emotion_detector_models\\model_59.hdf5\n",
      "56/56 [==============================] - 31s 549ms/step - loss: 1.5583 - accuracy: 0.4536 - val_loss: 1.6356 - val_accuracy: 0.4118\n",
      "Epoch 60/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5576 - accuracy: 0.4519\n",
      "Epoch 60: saving model to ./emotion_detector_models\\model_60.hdf5\n",
      "56/56 [==============================] - 32s 564ms/step - loss: 1.5576 - accuracy: 0.4519 - val_loss: 1.6731 - val_accuracy: 0.3940\n",
      "Epoch 61/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5473 - accuracy: 0.4549\n",
      "Epoch 61: saving model to ./emotion_detector_models\\model_61.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.5473 - accuracy: 0.4549 - val_loss: 1.5898 - val_accuracy: 0.4367\n",
      "Epoch 62/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5465 - accuracy: 0.4562\n",
      "Epoch 62: saving model to ./emotion_detector_models\\model_62.hdf5\n",
      "56/56 [==============================] - 32s 573ms/step - loss: 1.5465 - accuracy: 0.4562 - val_loss: 1.5855 - val_accuracy: 0.4261\n",
      "Epoch 63/100\n",
      "55/56 [============================>.] - ETA: 0s - loss: 1.5416 - accuracy: 0.4603\n",
      "Epoch 63: saving model to ./emotion_detector_models\\model_63.hdf5\n",
      "56/56 [==============================] - 31s 555ms/step - loss: 1.5417 - accuracy: 0.4601 - val_loss: 1.6047 - val_accuracy: 0.4261\n",
      "Epoch 64/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5301 - accuracy: 0.4631\n",
      "Epoch 64: saving model to ./emotion_detector_models\\model_64.hdf5\n",
      "56/56 [==============================] - 32s 563ms/step - loss: 1.5301 - accuracy: 0.4631 - val_loss: 1.6045 - val_accuracy: 0.4171\n",
      "Epoch 65/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5298 - accuracy: 0.4635\n",
      "Epoch 65: saving model to ./emotion_detector_models\\model_65.hdf5\n",
      "56/56 [==============================] - 31s 558ms/step - loss: 1.5298 - accuracy: 0.4635 - val_loss: 1.5852 - val_accuracy: 0.4280\n",
      "Epoch 66/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5247 - accuracy: 0.4646\n",
      "Epoch 66: saving model to ./emotion_detector_models\\model_66.hdf5\n",
      "56/56 [==============================] - 32s 559ms/step - loss: 1.5247 - accuracy: 0.4646 - val_loss: 1.5724 - val_accuracy: 0.4308\n",
      "Epoch 67/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5248 - accuracy: 0.4623\n",
      "Epoch 67: saving model to ./emotion_detector_models\\model_67.hdf5\n",
      "56/56 [==============================] - 32s 558ms/step - loss: 1.5248 - accuracy: 0.4623 - val_loss: 1.6072 - val_accuracy: 0.4166\n",
      "Epoch 68/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5146 - accuracy: 0.4658\n",
      "Epoch 68: saving model to ./emotion_detector_models\\model_68.hdf5\n",
      "56/56 [==============================] - 32s 572ms/step - loss: 1.5146 - accuracy: 0.4658 - val_loss: 1.5377 - val_accuracy: 0.4369\n",
      "Epoch 69/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5091 - accuracy: 0.4692\n",
      "Epoch 69: saving model to ./emotion_detector_models\\model_69.hdf5\n",
      "56/56 [==============================] - 31s 548ms/step - loss: 1.5091 - accuracy: 0.4692 - val_loss: 1.6483 - val_accuracy: 0.3951\n",
      "Epoch 70/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5071 - accuracy: 0.4693\n",
      "Epoch 70: saving model to ./emotion_detector_models\\model_70.hdf5\n",
      "56/56 [==============================] - 31s 546ms/step - loss: 1.5071 - accuracy: 0.4693 - val_loss: 1.5980 - val_accuracy: 0.4163\n",
      "Epoch 71/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5042 - accuracy: 0.4745\n",
      "Epoch 71: saving model to ./emotion_detector_models\\model_71.hdf5\n",
      "56/56 [==============================] - 31s 546ms/step - loss: 1.5042 - accuracy: 0.4745 - val_loss: 1.5940 - val_accuracy: 0.4099\n",
      "Epoch 72/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4982 - accuracy: 0.4698\n",
      "Epoch 72: saving model to ./emotion_detector_models\\model_72.hdf5\n",
      "56/56 [==============================] - 31s 554ms/step - loss: 1.4982 - accuracy: 0.4698 - val_loss: 1.5678 - val_accuracy: 0.4286\n",
      "Epoch 73/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4923 - accuracy: 0.4757\n",
      "Epoch 73: saving model to ./emotion_detector_models\\model_73.hdf5\n",
      "56/56 [==============================] - 33s 578ms/step - loss: 1.4923 - accuracy: 0.4757 - val_loss: 1.5896 - val_accuracy: 0.4205\n",
      "Epoch 74/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4911 - accuracy: 0.4777\n",
      "Epoch 74: saving model to ./emotion_detector_models\\model_74.hdf5\n",
      "56/56 [==============================] - 33s 590ms/step - loss: 1.4911 - accuracy: 0.4777 - val_loss: 1.5445 - val_accuracy: 0.4445\n",
      "Epoch 75/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4905 - accuracy: 0.4724\n",
      "Epoch 75: saving model to ./emotion_detector_models\\model_75.hdf5\n",
      "56/56 [==============================] - 32s 572ms/step - loss: 1.4905 - accuracy: 0.4724 - val_loss: 1.5728 - val_accuracy: 0.4294\n",
      "Epoch 76/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4816 - accuracy: 0.4783\n",
      "Epoch 76: saving model to ./emotion_detector_models\\model_76.hdf5\n",
      "56/56 [==============================] - 34s 607ms/step - loss: 1.4816 - accuracy: 0.4783 - val_loss: 1.5542 - val_accuracy: 0.4266\n",
      "Epoch 77/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4760 - accuracy: 0.4810\n",
      "Epoch 77: saving model to ./emotion_detector_models\\model_77.hdf5\n",
      "56/56 [==============================] - 32s 566ms/step - loss: 1.4760 - accuracy: 0.4810 - val_loss: 1.5758 - val_accuracy: 0.4280\n",
      "Epoch 78/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4757 - accuracy: 0.4805\n",
      "Epoch 78: saving model to ./emotion_detector_models\\model_78.hdf5\n",
      "56/56 [==============================] - 33s 580ms/step - loss: 1.4757 - accuracy: 0.4805 - val_loss: 1.5296 - val_accuracy: 0.4442\n",
      "Epoch 79/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4643 - accuracy: 0.4846\n",
      "Epoch 79: saving model to ./emotion_detector_models\\model_79.hdf5\n",
      "56/56 [==============================] - 33s 573ms/step - loss: 1.4643 - accuracy: 0.4846 - val_loss: 1.5530 - val_accuracy: 0.4275\n",
      "Epoch 80/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4613 - accuracy: 0.4860\n",
      "Epoch 80: saving model to ./emotion_detector_models\\model_80.hdf5\n",
      "56/56 [==============================] - 33s 581ms/step - loss: 1.4613 - accuracy: 0.4860 - val_loss: 1.5227 - val_accuracy: 0.4456\n",
      "Epoch 81/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4615 - accuracy: 0.4849\n",
      "Epoch 81: saving model to ./emotion_detector_models\\model_81.hdf5\n",
      "56/56 [==============================] - 33s 578ms/step - loss: 1.4615 - accuracy: 0.4849 - val_loss: 1.5590 - val_accuracy: 0.4322\n",
      "Epoch 82/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4575 - accuracy: 0.4813\n",
      "Epoch 82: saving model to ./emotion_detector_models\\model_82.hdf5\n",
      "56/56 [==============================] - 33s 586ms/step - loss: 1.4575 - accuracy: 0.4813 - val_loss: 1.5303 - val_accuracy: 0.4406\n",
      "Epoch 83/100\n",
      "55/56 [============================>.] - ETA: 0s - loss: 1.4603 - accuracy: 0.4854\n",
      "Epoch 83: saving model to ./emotion_detector_models\\model_83.hdf5\n",
      "56/56 [==============================] - 33s 580ms/step - loss: 1.4605 - accuracy: 0.4852 - val_loss: 1.5075 - val_accuracy: 0.4501\n",
      "Epoch 84/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4496 - accuracy: 0.4877\n",
      "Epoch 84: saving model to ./emotion_detector_models\\model_84.hdf5\n",
      "56/56 [==============================] - 34s 597ms/step - loss: 1.4496 - accuracy: 0.4877 - val_loss: 1.4827 - val_accuracy: 0.4570\n",
      "Epoch 85/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4535 - accuracy: 0.4861\n",
      "Epoch 85: saving model to ./emotion_detector_models\\model_85.hdf5\n",
      "56/56 [==============================] - 33s 588ms/step - loss: 1.4535 - accuracy: 0.4861 - val_loss: 1.5577 - val_accuracy: 0.4300\n",
      "Epoch 86/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4456 - accuracy: 0.4885\n",
      "Epoch 86: saving model to ./emotion_detector_models\\model_86.hdf5\n",
      "56/56 [==============================] - 34s 602ms/step - loss: 1.4456 - accuracy: 0.4885 - val_loss: 1.5478 - val_accuracy: 0.4302\n",
      "Epoch 87/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4381 - accuracy: 0.4920\n",
      "Epoch 87: saving model to ./emotion_detector_models\\model_87.hdf5\n",
      "56/56 [==============================] - 34s 614ms/step - loss: 1.4381 - accuracy: 0.4920 - val_loss: 1.5084 - val_accuracy: 0.4386\n",
      "Epoch 88/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4378 - accuracy: 0.4884\n",
      "Epoch 88: saving model to ./emotion_detector_models\\model_88.hdf5\n",
      "56/56 [==============================] - 34s 606ms/step - loss: 1.4378 - accuracy: 0.4884 - val_loss: 1.5284 - val_accuracy: 0.4333\n",
      "Epoch 89/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4372 - accuracy: 0.4904\n",
      "Epoch 89: saving model to ./emotion_detector_models\\model_89.hdf5\n",
      "56/56 [==============================] - 34s 592ms/step - loss: 1.4372 - accuracy: 0.4904 - val_loss: 1.5076 - val_accuracy: 0.4431\n",
      "Epoch 90/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4292 - accuracy: 0.4962\n",
      "Epoch 90: saving model to ./emotion_detector_models\\model_90.hdf5\n",
      "56/56 [==============================] - 32s 575ms/step - loss: 1.4292 - accuracy: 0.4962 - val_loss: 1.5077 - val_accuracy: 0.4425\n",
      "Epoch 91/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4263 - accuracy: 0.4957\n",
      "Epoch 91: saving model to ./emotion_detector_models\\model_91.hdf5\n",
      "56/56 [==============================] - 32s 565ms/step - loss: 1.4263 - accuracy: 0.4957 - val_loss: 1.5224 - val_accuracy: 0.4367\n",
      "Epoch 92/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4226 - accuracy: 0.4975\n",
      "Epoch 92: saving model to ./emotion_detector_models\\model_92.hdf5\n",
      "56/56 [==============================] - 33s 578ms/step - loss: 1.4226 - accuracy: 0.4975 - val_loss: 1.5332 - val_accuracy: 0.4300\n",
      "Epoch 93/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4154 - accuracy: 0.4968\n",
      "Epoch 93: saving model to ./emotion_detector_models\\model_93.hdf5\n",
      "56/56 [==============================] - 32s 570ms/step - loss: 1.4154 - accuracy: 0.4968 - val_loss: 1.4772 - val_accuracy: 0.4654\n",
      "Epoch 94/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4189 - accuracy: 0.4991\n",
      "Epoch 94: saving model to ./emotion_detector_models\\model_94.hdf5\n",
      "56/56 [==============================] - 32s 555ms/step - loss: 1.4189 - accuracy: 0.4991 - val_loss: 1.4810 - val_accuracy: 0.4565\n",
      "Epoch 95/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4158 - accuracy: 0.4970\n",
      "Epoch 95: saving model to ./emotion_detector_models\\model_95.hdf5\n",
      "56/56 [==============================] - 31s 556ms/step - loss: 1.4158 - accuracy: 0.4970 - val_loss: 1.4488 - val_accuracy: 0.4693\n",
      "Epoch 96/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4163 - accuracy: 0.4975\n",
      "Epoch 96: saving model to ./emotion_detector_models\\model_96.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 1.4163 - accuracy: 0.4975 - val_loss: 1.4888 - val_accuracy: 0.4548\n",
      "Epoch 97/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4102 - accuracy: 0.4997\n",
      "Epoch 97: saving model to ./emotion_detector_models\\model_97.hdf5\n",
      "56/56 [==============================] - 32s 559ms/step - loss: 1.4102 - accuracy: 0.4997 - val_loss: 1.4510 - val_accuracy: 0.4632\n",
      "Epoch 98/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4042 - accuracy: 0.5018\n",
      "Epoch 98: saving model to ./emotion_detector_models\\model_98.hdf5\n",
      "56/56 [==============================] - 32s 566ms/step - loss: 1.4042 - accuracy: 0.5018 - val_loss: 1.4634 - val_accuracy: 0.4559\n",
      "Epoch 99/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.3959 - accuracy: 0.5075\n",
      "Epoch 99: saving model to ./emotion_detector_models\\model_99.hdf5\n",
      "56/56 [==============================] - 32s 571ms/step - loss: 1.3959 - accuracy: 0.5075 - val_loss: 1.4694 - val_accuracy: 0.4540\n",
      "Epoch 100/100\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.4012 - accuracy: 0.4971\n",
      "Epoch 100: saving model to ./emotion_detector_models\\model_100.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.4012 - accuracy: 0.4971 - val_loss: 1.4977 - val_accuracy: 0.4375\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_model_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-66d5238aa4a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m             validation_steps=nb_validation_samples // batch_size)\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mplot_model_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_model_history' is not defined"
     ]
    }
   ],
   "source": [
    "# If you want to train the same model or try other models, go for this\n",
    "\n",
    "\n",
    "filepath = os.path.join(\"./emotion_detector_models/model_{epoch}.hdf5\")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                             monitor='val_acc',\n",
    "                                             verbose=1,\n",
    "                                             save_best_only=False,\n",
    "                                             mode='max')\n",
    "callbacks = [checkpoint]\n",
    "# if mode == \"train\":\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "nb_train_samples = 28709\n",
    "nb_validation_samples = 3589\n",
    "epochs = 100\n",
    "model_info = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks = callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "# plot_model_history(model_info)\n",
    "# model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055e8f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model_info.history.keys())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_info.history['loss'])\n",
    "plt.plot(model_info.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d403373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(\"./emotion_detector_models/model_100.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0fb859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1335 - accuracy: 0.5903\n",
      "Epoch 1: saving model to ./emotion_detector_models\\model_1.hdf5\n",
      "56/56 [==============================] - 31s 538ms/step - loss: 1.1335 - accuracy: 0.5903 - val_loss: 1.3401 - val_accuracy: 0.4961\n",
      "Epoch 2/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1323 - accuracy: 0.5897\n",
      "Epoch 2: saving model to ./emotion_detector_models\\model_2.hdf5\n",
      "56/56 [==============================] - 31s 553ms/step - loss: 1.1323 - accuracy: 0.5897 - val_loss: 1.2613 - val_accuracy: 0.5290\n",
      "Epoch 3/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1308 - accuracy: 0.5884\n",
      "Epoch 3: saving model to ./emotion_detector_models\\model_3.hdf5\n",
      "56/56 [==============================] - 30s 534ms/step - loss: 1.1308 - accuracy: 0.5884 - val_loss: 1.2934 - val_accuracy: 0.5190\n",
      "Epoch 4/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1292 - accuracy: 0.5901\n",
      "Epoch 4: saving model to ./emotion_detector_models\\model_4.hdf5\n",
      "56/56 [==============================] - 31s 548ms/step - loss: 1.1292 - accuracy: 0.5901 - val_loss: 1.2836 - val_accuracy: 0.5176\n",
      "Epoch 5/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1325 - accuracy: 0.5887\n",
      "Epoch 5: saving model to ./emotion_detector_models\\model_5.hdf5\n",
      "56/56 [==============================] - 32s 559ms/step - loss: 1.1325 - accuracy: 0.5887 - val_loss: 1.2874 - val_accuracy: 0.5170\n",
      "Epoch 6/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1247 - accuracy: 0.5933\n",
      "Epoch 6: saving model to ./emotion_detector_models\\model_6.hdf5\n",
      "56/56 [==============================] - 32s 574ms/step - loss: 1.1247 - accuracy: 0.5933 - val_loss: 1.2714 - val_accuracy: 0.5173\n",
      "Epoch 7/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1229 - accuracy: 0.5953\n",
      "Epoch 7: saving model to ./emotion_detector_models\\model_7.hdf5\n",
      "56/56 [==============================] - 32s 569ms/step - loss: 1.1229 - accuracy: 0.5953 - val_loss: 1.2281 - val_accuracy: 0.5340\n",
      "Epoch 8/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1253 - accuracy: 0.5916\n",
      "Epoch 8: saving model to ./emotion_detector_models\\model_8.hdf5\n",
      "56/56 [==============================] - 36s 637ms/step - loss: 1.1253 - accuracy: 0.5916 - val_loss: 1.2882 - val_accuracy: 0.5162\n",
      "Epoch 9/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1171 - accuracy: 0.5957\n",
      "Epoch 9: saving model to ./emotion_detector_models\\model_9.hdf5\n",
      "56/56 [==============================] - 41s 723ms/step - loss: 1.1171 - accuracy: 0.5957 - val_loss: 1.2756 - val_accuracy: 0.5212\n",
      "Epoch 10/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1173 - accuracy: 0.5936\n",
      "Epoch 10: saving model to ./emotion_detector_models\\model_10.hdf5\n",
      "56/56 [==============================] - 34s 599ms/step - loss: 1.1173 - accuracy: 0.5936 - val_loss: 1.2710 - val_accuracy: 0.5131\n",
      "Epoch 11/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1110 - accuracy: 0.5960\n",
      "Epoch 11: saving model to ./emotion_detector_models\\model_11.hdf5\n",
      "56/56 [==============================] - 36s 630ms/step - loss: 1.1110 - accuracy: 0.5960 - val_loss: 1.2887 - val_accuracy: 0.5187\n",
      "Epoch 12/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1166 - accuracy: 0.5935\n",
      "Epoch 12: saving model to ./emotion_detector_models\\model_12.hdf5\n",
      "56/56 [==============================] - 33s 584ms/step - loss: 1.1166 - accuracy: 0.5935 - val_loss: 1.2775 - val_accuracy: 0.5262\n",
      "Epoch 13/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1116 - accuracy: 0.5953\n",
      "Epoch 13: saving model to ./emotion_detector_models\\model_13.hdf5\n",
      "56/56 [==============================] - 33s 582ms/step - loss: 1.1116 - accuracy: 0.5953 - val_loss: 1.2667 - val_accuracy: 0.5296\n",
      "Epoch 14/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1140 - accuracy: 0.5950\n",
      "Epoch 14: saving model to ./emotion_detector_models\\model_14.hdf5\n",
      "56/56 [==============================] - 34s 605ms/step - loss: 1.1140 - accuracy: 0.5950 - val_loss: 1.2757 - val_accuracy: 0.5128\n",
      "Epoch 15/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1125 - accuracy: 0.5983\n",
      "Epoch 15: saving model to ./emotion_detector_models\\model_15.hdf5\n",
      "56/56 [==============================] - 32s 570ms/step - loss: 1.1125 - accuracy: 0.5983 - val_loss: 1.2295 - val_accuracy: 0.5349\n",
      "Epoch 16/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1104 - accuracy: 0.5956\n",
      "Epoch 16: saving model to ./emotion_detector_models\\model_16.hdf5\n",
      "56/56 [==============================] - 33s 588ms/step - loss: 1.1104 - accuracy: 0.5956 - val_loss: 1.2726 - val_accuracy: 0.5259\n",
      "Epoch 17/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1079 - accuracy: 0.6007\n",
      "Epoch 17: saving model to ./emotion_detector_models\\model_17.hdf5\n",
      "56/56 [==============================] - 34s 596ms/step - loss: 1.1079 - accuracy: 0.6007 - val_loss: 1.2656 - val_accuracy: 0.5215\n",
      "Epoch 18/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1021 - accuracy: 0.6044\n",
      "Epoch 18: saving model to ./emotion_detector_models\\model_18.hdf5\n",
      "56/56 [==============================] - 33s 591ms/step - loss: 1.1021 - accuracy: 0.6044 - val_loss: 1.2408 - val_accuracy: 0.5360\n",
      "Epoch 19/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1044 - accuracy: 0.5985\n",
      "Epoch 19: saving model to ./emotion_detector_models\\model_19.hdf5\n",
      "56/56 [==============================] - 33s 582ms/step - loss: 1.1044 - accuracy: 0.5985 - val_loss: 1.2475 - val_accuracy: 0.5318\n",
      "Epoch 20/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1090 - accuracy: 0.5960\n",
      "Epoch 20: saving model to ./emotion_detector_models\\model_20.hdf5\n",
      "56/56 [==============================] - 35s 612ms/step - loss: 1.1090 - accuracy: 0.5960 - val_loss: 1.2570 - val_accuracy: 0.5240\n",
      "Epoch 21/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1075 - accuracy: 0.5970\n",
      "Epoch 21: saving model to ./emotion_detector_models\\model_21.hdf5\n",
      "56/56 [==============================] - 35s 615ms/step - loss: 1.1075 - accuracy: 0.5970 - val_loss: 1.3236 - val_accuracy: 0.5025\n",
      "Epoch 22/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1045 - accuracy: 0.5994\n",
      "Epoch 22: saving model to ./emotion_detector_models\\model_22.hdf5\n",
      "56/56 [==============================] - 34s 598ms/step - loss: 1.1045 - accuracy: 0.5994 - val_loss: 1.2243 - val_accuracy: 0.5371\n",
      "Epoch 23/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1038 - accuracy: 0.6011\n",
      "Epoch 23: saving model to ./emotion_detector_models\\model_23.hdf5\n",
      "56/56 [==============================] - 33s 582ms/step - loss: 1.1038 - accuracy: 0.6011 - val_loss: 1.2896 - val_accuracy: 0.5134\n",
      "Epoch 24/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0965 - accuracy: 0.6018\n",
      "Epoch 24: saving model to ./emotion_detector_models\\model_24.hdf5\n",
      "56/56 [==============================] - 33s 574ms/step - loss: 1.0965 - accuracy: 0.6018 - val_loss: 1.3116 - val_accuracy: 0.5045\n",
      "Epoch 25/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0980 - accuracy: 0.6045\n",
      "Epoch 25: saving model to ./emotion_detector_models\\model_25.hdf5\n",
      "56/56 [==============================] - 35s 611ms/step - loss: 1.0980 - accuracy: 0.6045 - val_loss: 1.2029 - val_accuracy: 0.5516\n",
      "Epoch 26/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0970 - accuracy: 0.6038\n",
      "Epoch 26: saving model to ./emotion_detector_models\\model_26.hdf5\n",
      "56/56 [==============================] - 34s 595ms/step - loss: 1.0970 - accuracy: 0.6038 - val_loss: 1.3018 - val_accuracy: 0.5000\n",
      "Epoch 27/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.1046 - accuracy: 0.6000\n",
      "Epoch 27: saving model to ./emotion_detector_models\\model_27.hdf5\n",
      "56/56 [==============================] - 34s 594ms/step - loss: 1.1046 - accuracy: 0.6000 - val_loss: 1.2297 - val_accuracy: 0.5329\n",
      "Epoch 28/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0969 - accuracy: 0.6020\n",
      "Epoch 28: saving model to ./emotion_detector_models\\model_28.hdf5\n",
      "56/56 [==============================] - 33s 587ms/step - loss: 1.0969 - accuracy: 0.6020 - val_loss: 1.3260 - val_accuracy: 0.5064\n",
      "Epoch 29/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0926 - accuracy: 0.6068\n",
      "Epoch 29: saving model to ./emotion_detector_models\\model_29.hdf5\n",
      "56/56 [==============================] - 35s 613ms/step - loss: 1.0926 - accuracy: 0.6068 - val_loss: 1.2613 - val_accuracy: 0.5287\n",
      "Epoch 30/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0969 - accuracy: 0.6041\n",
      "Epoch 30: saving model to ./emotion_detector_models\\model_30.hdf5\n",
      "56/56 [==============================] - 34s 596ms/step - loss: 1.0969 - accuracy: 0.6041 - val_loss: 1.2196 - val_accuracy: 0.5382\n",
      "Epoch 31/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0867 - accuracy: 0.6063\n",
      "Epoch 31: saving model to ./emotion_detector_models\\model_31.hdf5\n",
      "56/56 [==============================] - 34s 609ms/step - loss: 1.0867 - accuracy: 0.6063 - val_loss: 1.2344 - val_accuracy: 0.5357\n",
      "Epoch 32/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0893 - accuracy: 0.6052\n",
      "Epoch 32: saving model to ./emotion_detector_models\\model_32.hdf5\n",
      "56/56 [==============================] - 34s 594ms/step - loss: 1.0893 - accuracy: 0.6052 - val_loss: 1.2643 - val_accuracy: 0.5229\n",
      "Epoch 33/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0853 - accuracy: 0.6064\n",
      "Epoch 33: saving model to ./emotion_detector_models\\model_33.hdf5\n",
      "56/56 [==============================] - 33s 578ms/step - loss: 1.0853 - accuracy: 0.6064 - val_loss: 1.2428 - val_accuracy: 0.5402\n",
      "Epoch 34/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0857 - accuracy: 0.6057\n",
      "Epoch 34: saving model to ./emotion_detector_models\\model_34.hdf5\n",
      "56/56 [==============================] - 34s 602ms/step - loss: 1.0857 - accuracy: 0.6057 - val_loss: 1.2532 - val_accuracy: 0.5290\n",
      "Epoch 35/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0821 - accuracy: 0.6092\n",
      "Epoch 35: saving model to ./emotion_detector_models\\model_35.hdf5\n",
      "56/56 [==============================] - 34s 600ms/step - loss: 1.0821 - accuracy: 0.6092 - val_loss: 1.3096 - val_accuracy: 0.5064\n",
      "Epoch 36/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0865 - accuracy: 0.6056\n",
      "Epoch 36: saving model to ./emotion_detector_models\\model_36.hdf5\n",
      "56/56 [==============================] - 35s 612ms/step - loss: 1.0865 - accuracy: 0.6056 - val_loss: 1.2383 - val_accuracy: 0.5393\n",
      "Epoch 37/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0901 - accuracy: 0.6045\n",
      "Epoch 37: saving model to ./emotion_detector_models\\model_37.hdf5\n",
      "56/56 [==============================] - 33s 589ms/step - loss: 1.0901 - accuracy: 0.6045 - val_loss: 1.2692 - val_accuracy: 0.5326\n",
      "Epoch 38/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0817 - accuracy: 0.6094\n",
      "Epoch 38: saving model to ./emotion_detector_models\\model_38.hdf5\n",
      "56/56 [==============================] - 34s 600ms/step - loss: 1.0817 - accuracy: 0.6094 - val_loss: 1.2113 - val_accuracy: 0.5430\n",
      "Epoch 39/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0801 - accuracy: 0.6075\n",
      "Epoch 39: saving model to ./emotion_detector_models\\model_39.hdf5\n",
      "56/56 [==============================] - 33s 587ms/step - loss: 1.0801 - accuracy: 0.6075 - val_loss: 1.2638 - val_accuracy: 0.5254\n",
      "Epoch 40/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0811 - accuracy: 0.6046\n",
      "Epoch 40: saving model to ./emotion_detector_models\\model_40.hdf5\n",
      "56/56 [==============================] - 33s 591ms/step - loss: 1.0811 - accuracy: 0.6046 - val_loss: 1.2792 - val_accuracy: 0.5237\n",
      "Epoch 41/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0861 - accuracy: 0.6057\n",
      "Epoch 41: saving model to ./emotion_detector_models\\model_41.hdf5\n",
      "56/56 [==============================] - 34s 591ms/step - loss: 1.0861 - accuracy: 0.6057 - val_loss: 1.3053 - val_accuracy: 0.5123\n",
      "Epoch 42/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0848 - accuracy: 0.6044\n",
      "Epoch 42: saving model to ./emotion_detector_models\\model_42.hdf5\n",
      "56/56 [==============================] - 33s 591ms/step - loss: 1.0848 - accuracy: 0.6044 - val_loss: 1.2049 - val_accuracy: 0.5555\n",
      "Epoch 43/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0856 - accuracy: 0.6048\n",
      "Epoch 43: saving model to ./emotion_detector_models\\model_43.hdf5\n",
      "56/56 [==============================] - 35s 618ms/step - loss: 1.0856 - accuracy: 0.6048 - val_loss: 1.2075 - val_accuracy: 0.5435\n",
      "Epoch 44/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0819 - accuracy: 0.6038\n",
      "Epoch 44: saving model to ./emotion_detector_models\\model_44.hdf5\n",
      "56/56 [==============================] - 37s 657ms/step - loss: 1.0819 - accuracy: 0.6038 - val_loss: 1.3001 - val_accuracy: 0.5193\n",
      "Epoch 45/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0774 - accuracy: 0.6086\n",
      "Epoch 45: saving model to ./emotion_detector_models\\model_45.hdf5\n",
      "56/56 [==============================] - 33s 584ms/step - loss: 1.0774 - accuracy: 0.6086 - val_loss: 1.2639 - val_accuracy: 0.5240\n",
      "Epoch 46/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0767 - accuracy: 0.6118\n",
      "Epoch 46: saving model to ./emotion_detector_models\\model_46.hdf5\n",
      "56/56 [==============================] - 33s 581ms/step - loss: 1.0767 - accuracy: 0.6118 - val_loss: 1.2930 - val_accuracy: 0.5243\n",
      "Epoch 47/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0785 - accuracy: 0.6082\n",
      "Epoch 47: saving model to ./emotion_detector_models\\model_47.hdf5\n",
      "56/56 [==============================] - 33s 591ms/step - loss: 1.0785 - accuracy: 0.6082 - val_loss: 1.3110 - val_accuracy: 0.5140\n",
      "Epoch 48/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0704 - accuracy: 0.6127\n",
      "Epoch 48: saving model to ./emotion_detector_models\\model_48.hdf5\n",
      "56/56 [==============================] - 33s 591ms/step - loss: 1.0704 - accuracy: 0.6127 - val_loss: 1.2780 - val_accuracy: 0.5218\n",
      "Epoch 49/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0737 - accuracy: 0.6110\n",
      "Epoch 49: saving model to ./emotion_detector_models\\model_49.hdf5\n",
      "56/56 [==============================] - 33s 583ms/step - loss: 1.0737 - accuracy: 0.6110 - val_loss: 1.2574 - val_accuracy: 0.5296\n",
      "Epoch 50/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0740 - accuracy: 0.6104\n",
      "Epoch 50: saving model to ./emotion_detector_models\\model_50.hdf5\n",
      "56/56 [==============================] - 34s 600ms/step - loss: 1.0740 - accuracy: 0.6104 - val_loss: 1.2947 - val_accuracy: 0.5081\n",
      "Epoch 51/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0722 - accuracy: 0.6138\n",
      "Epoch 51: saving model to ./emotion_detector_models\\model_51.hdf5\n",
      "56/56 [==============================] - 34s 598ms/step - loss: 1.0722 - accuracy: 0.6138 - val_loss: 1.2251 - val_accuracy: 0.5377\n",
      "Epoch 52/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0712 - accuracy: 0.6124\n",
      "Epoch 52: saving model to ./emotion_detector_models\\model_52.hdf5\n",
      "56/56 [==============================] - 34s 605ms/step - loss: 1.0712 - accuracy: 0.6124 - val_loss: 1.2391 - val_accuracy: 0.5396\n",
      "Epoch 53/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0735 - accuracy: 0.6092\n",
      "Epoch 53: saving model to ./emotion_detector_models\\model_53.hdf5\n",
      "56/56 [==============================] - 33s 589ms/step - loss: 1.0735 - accuracy: 0.6092 - val_loss: 1.2532 - val_accuracy: 0.5363\n",
      "Epoch 54/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0696 - accuracy: 0.6125\n",
      "Epoch 54: saving model to ./emotion_detector_models\\model_54.hdf5\n",
      "56/56 [==============================] - 34s 604ms/step - loss: 1.0696 - accuracy: 0.6125 - val_loss: 1.2367 - val_accuracy: 0.5276\n",
      "Epoch 55/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0647 - accuracy: 0.6165\n",
      "Epoch 55: saving model to ./emotion_detector_models\\model_55.hdf5\n",
      "56/56 [==============================] - 34s 596ms/step - loss: 1.0647 - accuracy: 0.6165 - val_loss: 1.1880 - val_accuracy: 0.5494\n",
      "Epoch 56/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0647 - accuracy: 0.6118\n",
      "Epoch 56: saving model to ./emotion_detector_models\\model_56.hdf5\n",
      "56/56 [==============================] - 33s 592ms/step - loss: 1.0647 - accuracy: 0.6118 - val_loss: 1.2253 - val_accuracy: 0.5396\n",
      "Epoch 57/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0632 - accuracy: 0.6132\n",
      "Epoch 57: saving model to ./emotion_detector_models\\model_57.hdf5\n",
      "56/56 [==============================] - 35s 625ms/step - loss: 1.0632 - accuracy: 0.6132 - val_loss: 1.2362 - val_accuracy: 0.5343\n",
      "Epoch 58/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0673 - accuracy: 0.6150\n",
      "Epoch 58: saving model to ./emotion_detector_models\\model_58.hdf5\n",
      "56/56 [==============================] - 32s 559ms/step - loss: 1.0673 - accuracy: 0.6150 - val_loss: 1.2386 - val_accuracy: 0.5335\n",
      "Epoch 59/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0616 - accuracy: 0.6153\n",
      "Epoch 59: saving model to ./emotion_detector_models\\model_59.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.0616 - accuracy: 0.6153 - val_loss: 1.3017 - val_accuracy: 0.5243\n",
      "Epoch 60/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0649 - accuracy: 0.6147\n",
      "Epoch 60: saving model to ./emotion_detector_models\\model_60.hdf5\n",
      "56/56 [==============================] - 32s 558ms/step - loss: 1.0649 - accuracy: 0.6147 - val_loss: 1.2414 - val_accuracy: 0.5324\n",
      "Epoch 61/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0635 - accuracy: 0.6154\n",
      "Epoch 61: saving model to ./emotion_detector_models\\model_61.hdf5\n",
      "56/56 [==============================] - 33s 581ms/step - loss: 1.0635 - accuracy: 0.6154 - val_loss: 1.2404 - val_accuracy: 0.5312\n",
      "Epoch 62/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0624 - accuracy: 0.6145\n",
      "Epoch 62: saving model to ./emotion_detector_models\\model_62.hdf5\n",
      "56/56 [==============================] - 31s 551ms/step - loss: 1.0624 - accuracy: 0.6145 - val_loss: 1.2162 - val_accuracy: 0.5491\n",
      "Epoch 63/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0547 - accuracy: 0.6159\n",
      "Epoch 63: saving model to ./emotion_detector_models\\model_63.hdf5\n",
      "56/56 [==============================] - 32s 556ms/step - loss: 1.0547 - accuracy: 0.6159 - val_loss: 1.1858 - val_accuracy: 0.5519\n",
      "Epoch 64/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0590 - accuracy: 0.6178\n",
      "Epoch 64: saving model to ./emotion_detector_models\\model_64.hdf5\n",
      "56/56 [==============================] - 31s 554ms/step - loss: 1.0590 - accuracy: 0.6178 - val_loss: 1.2776 - val_accuracy: 0.5220\n",
      "Epoch 65/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0588 - accuracy: 0.6166\n",
      "Epoch 65: saving model to ./emotion_detector_models\\model_65.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.0588 - accuracy: 0.6166 - val_loss: 1.2275 - val_accuracy: 0.5452\n",
      "Epoch 66/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0593 - accuracy: 0.6178\n",
      "Epoch 66: saving model to ./emotion_detector_models\\model_66.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 1.0593 - accuracy: 0.6178 - val_loss: 1.2403 - val_accuracy: 0.5310\n",
      "Epoch 67/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0578 - accuracy: 0.6142\n",
      "Epoch 67: saving model to ./emotion_detector_models\\model_67.hdf5\n",
      "56/56 [==============================] - 31s 555ms/step - loss: 1.0578 - accuracy: 0.6142 - val_loss: 1.3058 - val_accuracy: 0.5131\n",
      "Epoch 68/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0560 - accuracy: 0.6168\n",
      "Epoch 68: saving model to ./emotion_detector_models\\model_68.hdf5\n",
      "56/56 [==============================] - 31s 552ms/step - loss: 1.0560 - accuracy: 0.6168 - val_loss: 1.2788 - val_accuracy: 0.5237\n",
      "Epoch 69/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0540 - accuracy: 0.6168\n",
      "Epoch 69: saving model to ./emotion_detector_models\\model_69.hdf5\n",
      "56/56 [==============================] - 31s 554ms/step - loss: 1.0540 - accuracy: 0.6168 - val_loss: 1.2845 - val_accuracy: 0.5167\n",
      "Epoch 70/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0558 - accuracy: 0.6155\n",
      "Epoch 70: saving model to ./emotion_detector_models\\model_70.hdf5\n",
      "56/56 [==============================] - 32s 567ms/step - loss: 1.0558 - accuracy: 0.6155 - val_loss: 1.2673 - val_accuracy: 0.5240\n",
      "Epoch 71/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0566 - accuracy: 0.6183\n",
      "Epoch 71: saving model to ./emotion_detector_models\\model_71.hdf5\n",
      "56/56 [==============================] - 31s 555ms/step - loss: 1.0566 - accuracy: 0.6183 - val_loss: 1.2768 - val_accuracy: 0.5223\n",
      "Epoch 72/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0583 - accuracy: 0.6164\n",
      "Epoch 72: saving model to ./emotion_detector_models\\model_72.hdf5\n",
      "56/56 [==============================] - 32s 556ms/step - loss: 1.0583 - accuracy: 0.6164 - val_loss: 1.2345 - val_accuracy: 0.5374\n",
      "Epoch 73/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0556 - accuracy: 0.6134\n",
      "Epoch 73: saving model to ./emotion_detector_models\\model_73.hdf5\n",
      "56/56 [==============================] - 31s 547ms/step - loss: 1.0556 - accuracy: 0.6134 - val_loss: 1.2580 - val_accuracy: 0.5304\n",
      "Epoch 74/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0437 - accuracy: 0.6218\n",
      "Epoch 74: saving model to ./emotion_detector_models\\model_74.hdf5\n",
      "56/56 [==============================] - 31s 555ms/step - loss: 1.0437 - accuracy: 0.6218 - val_loss: 1.2782 - val_accuracy: 0.5187\n",
      "Epoch 75/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0521 - accuracy: 0.6193\n",
      "Epoch 75: saving model to ./emotion_detector_models\\model_75.hdf5\n",
      "56/56 [==============================] - 31s 551ms/step - loss: 1.0521 - accuracy: 0.6193 - val_loss: 1.2147 - val_accuracy: 0.5485\n",
      "Epoch 76/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0491 - accuracy: 0.6168\n",
      "Epoch 76: saving model to ./emotion_detector_models\\model_76.hdf5\n",
      "56/56 [==============================] - 32s 565ms/step - loss: 1.0491 - accuracy: 0.6168 - val_loss: 1.1976 - val_accuracy: 0.5558\n",
      "Epoch 77/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0477 - accuracy: 0.6205\n",
      "Epoch 77: saving model to ./emotion_detector_models\\model_77.hdf5\n",
      "56/56 [==============================] - 31s 553ms/step - loss: 1.0477 - accuracy: 0.6205 - val_loss: 1.2137 - val_accuracy: 0.5379\n",
      "Epoch 78/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0514 - accuracy: 0.6172\n",
      "Epoch 78: saving model to ./emotion_detector_models\\model_78.hdf5\n",
      "56/56 [==============================] - 32s 561ms/step - loss: 1.0514 - accuracy: 0.6172 - val_loss: 1.2087 - val_accuracy: 0.5430\n",
      "Epoch 79/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0471 - accuracy: 0.6217\n",
      "Epoch 79: saving model to ./emotion_detector_models\\model_79.hdf5\n",
      "56/56 [==============================] - 33s 575ms/step - loss: 1.0471 - accuracy: 0.6217 - val_loss: 1.2241 - val_accuracy: 0.5402\n",
      "Epoch 80/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0448 - accuracy: 0.6198\n",
      "Epoch 80: saving model to ./emotion_detector_models\\model_80.hdf5\n",
      "56/56 [==============================] - 33s 587ms/step - loss: 1.0448 - accuracy: 0.6198 - val_loss: 1.2457 - val_accuracy: 0.5349\n",
      "Epoch 81/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0444 - accuracy: 0.6206\n",
      "Epoch 81: saving model to ./emotion_detector_models\\model_81.hdf5\n",
      "56/56 [==============================] - 32s 566ms/step - loss: 1.0444 - accuracy: 0.6206 - val_loss: 1.3007 - val_accuracy: 0.5170\n",
      "Epoch 82/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0440 - accuracy: 0.6223\n",
      "Epoch 82: saving model to ./emotion_detector_models\\model_82.hdf5\n",
      "56/56 [==============================] - 32s 571ms/step - loss: 1.0440 - accuracy: 0.6223 - val_loss: 1.2477 - val_accuracy: 0.5332\n",
      "Epoch 83/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0421 - accuracy: 0.6206\n",
      "Epoch 83: saving model to ./emotion_detector_models\\model_83.hdf5\n",
      "56/56 [==============================] - 32s 565ms/step - loss: 1.0421 - accuracy: 0.6206 - val_loss: 1.2633 - val_accuracy: 0.5282\n",
      "Epoch 84/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0400 - accuracy: 0.6253\n",
      "Epoch 84: saving model to ./emotion_detector_models\\model_84.hdf5\n",
      "56/56 [==============================] - 32s 571ms/step - loss: 1.0400 - accuracy: 0.6253 - val_loss: 1.2077 - val_accuracy: 0.5499\n",
      "Epoch 85/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0419 - accuracy: 0.6217\n",
      "Epoch 85: saving model to ./emotion_detector_models\\model_85.hdf5\n",
      "56/56 [==============================] - 32s 565ms/step - loss: 1.0419 - accuracy: 0.6217 - val_loss: 1.2306 - val_accuracy: 0.5388\n",
      "Epoch 86/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0390 - accuracy: 0.6212\n",
      "Epoch 86: saving model to ./emotion_detector_models\\model_86.hdf5\n",
      "56/56 [==============================] - 31s 549ms/step - loss: 1.0390 - accuracy: 0.6212 - val_loss: 1.2405 - val_accuracy: 0.5402\n",
      "Epoch 87/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0417 - accuracy: 0.6205\n",
      "Epoch 87: saving model to ./emotion_detector_models\\model_87.hdf5\n",
      "56/56 [==============================] - 32s 556ms/step - loss: 1.0417 - accuracy: 0.6205 - val_loss: 1.2527 - val_accuracy: 0.5391\n",
      "Epoch 88/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0362 - accuracy: 0.6248\n",
      "Epoch 88: saving model to ./emotion_detector_models\\model_88.hdf5\n",
      "56/56 [==============================] - 32s 559ms/step - loss: 1.0362 - accuracy: 0.6248 - val_loss: 1.2894 - val_accuracy: 0.5167\n",
      "Epoch 89/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0319 - accuracy: 0.6233\n",
      "Epoch 89: saving model to ./emotion_detector_models\\model_89.hdf5\n",
      "56/56 [==============================] - 32s 565ms/step - loss: 1.0319 - accuracy: 0.6233 - val_loss: 1.2374 - val_accuracy: 0.5419\n",
      "Epoch 90/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0371 - accuracy: 0.6245\n",
      "Epoch 90: saving model to ./emotion_detector_models\\model_90.hdf5\n",
      "56/56 [==============================] - 32s 573ms/step - loss: 1.0371 - accuracy: 0.6245 - val_loss: 1.2683 - val_accuracy: 0.5257\n",
      "Epoch 91/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0411 - accuracy: 0.6201\n",
      "Epoch 91: saving model to ./emotion_detector_models\\model_91.hdf5\n",
      "56/56 [==============================] - 32s 563ms/step - loss: 1.0411 - accuracy: 0.6201 - val_loss: 1.2582 - val_accuracy: 0.5254\n",
      "Epoch 92/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0351 - accuracy: 0.6230\n",
      "Epoch 92: saving model to ./emotion_detector_models\\model_92.hdf5\n",
      "56/56 [==============================] - 32s 556ms/step - loss: 1.0351 - accuracy: 0.6230 - val_loss: 1.1634 - val_accuracy: 0.5678\n",
      "Epoch 93/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0332 - accuracy: 0.6243\n",
      "Epoch 93: saving model to ./emotion_detector_models\\model_93.hdf5\n",
      "56/56 [==============================] - 31s 554ms/step - loss: 1.0332 - accuracy: 0.6243 - val_loss: 1.2233 - val_accuracy: 0.5391\n",
      "Epoch 94/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0381 - accuracy: 0.6215\n",
      "Epoch 94: saving model to ./emotion_detector_models\\model_94.hdf5\n",
      "56/56 [==============================] - 31s 555ms/step - loss: 1.0381 - accuracy: 0.6215 - val_loss: 1.1732 - val_accuracy: 0.5622\n",
      "Epoch 95/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0374 - accuracy: 0.6219\n",
      "Epoch 95: saving model to ./emotion_detector_models\\model_95.hdf5\n",
      "56/56 [==============================] - 31s 570ms/step - loss: 1.0374 - accuracy: 0.6219 - val_loss: 1.2181 - val_accuracy: 0.5458\n",
      "Epoch 96/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0339 - accuracy: 0.6238\n",
      "Epoch 96: saving model to ./emotion_detector_models\\model_96.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 1.0339 - accuracy: 0.6238 - val_loss: 1.2786 - val_accuracy: 0.5153\n",
      "Epoch 97/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0351 - accuracy: 0.6240\n",
      "Epoch 97: saving model to ./emotion_detector_models\\model_97.hdf5\n",
      "56/56 [==============================] - 33s 581ms/step - loss: 1.0351 - accuracy: 0.6240 - val_loss: 1.2713 - val_accuracy: 0.5262\n",
      "Epoch 98/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0303 - accuracy: 0.6211\n",
      "Epoch 98: saving model to ./emotion_detector_models\\model_98.hdf5\n",
      "56/56 [==============================] - 32s 568ms/step - loss: 1.0303 - accuracy: 0.6211 - val_loss: 1.2311 - val_accuracy: 0.5430\n",
      "Epoch 99/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0289 - accuracy: 0.6286\n",
      "Epoch 99: saving model to ./emotion_detector_models\\model_99.hdf5\n",
      "56/56 [==============================] - 32s 569ms/step - loss: 1.0289 - accuracy: 0.6286 - val_loss: 1.2157 - val_accuracy: 0.5460\n",
      "Epoch 100/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0289 - accuracy: 0.6247\n",
      "Epoch 100: saving model to ./emotion_detector_models\\model_100.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 1.0289 - accuracy: 0.6247 - val_loss: 1.1818 - val_accuracy: 0.5689\n",
      "Epoch 101/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0306 - accuracy: 0.6250\n",
      "Epoch 101: saving model to ./emotion_detector_models\\model_101.hdf5\n",
      "56/56 [==============================] - 32s 560ms/step - loss: 1.0306 - accuracy: 0.6250 - val_loss: 1.1841 - val_accuracy: 0.5536\n",
      "Epoch 102/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0292 - accuracy: 0.6257\n",
      "Epoch 102: saving model to ./emotion_detector_models\\model_102.hdf5\n",
      "56/56 [==============================] - 32s 558ms/step - loss: 1.0292 - accuracy: 0.6257 - val_loss: 1.2394 - val_accuracy: 0.5352\n",
      "Epoch 103/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0288 - accuracy: 0.6270\n",
      "Epoch 103: saving model to ./emotion_detector_models\\model_103.hdf5\n",
      "56/56 [==============================] - 32s 560ms/step - loss: 1.0288 - accuracy: 0.6270 - val_loss: 1.2548 - val_accuracy: 0.5279\n",
      "Epoch 104/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0271 - accuracy: 0.6249\n",
      "Epoch 104: saving model to ./emotion_detector_models\\model_104.hdf5\n",
      "56/56 [==============================] - 32s 566ms/step - loss: 1.0271 - accuracy: 0.6249 - val_loss: 1.2589 - val_accuracy: 0.5223\n",
      "Epoch 105/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0235 - accuracy: 0.6273\n",
      "Epoch 105: saving model to ./emotion_detector_models\\model_105.hdf5\n",
      "56/56 [==============================] - 33s 574ms/step - loss: 1.0235 - accuracy: 0.6273 - val_loss: 1.2659 - val_accuracy: 0.5338\n",
      "Epoch 106/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0300 - accuracy: 0.6249\n",
      "Epoch 106: saving model to ./emotion_detector_models\\model_106.hdf5\n",
      "56/56 [==============================] - 32s 569ms/step - loss: 1.0300 - accuracy: 0.6249 - val_loss: 1.2589 - val_accuracy: 0.5324\n",
      "Epoch 107/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0210 - accuracy: 0.6245\n",
      "Epoch 107: saving model to ./emotion_detector_models\\model_107.hdf5\n",
      "56/56 [==============================] - 31s 550ms/step - loss: 1.0210 - accuracy: 0.6245 - val_loss: 1.1800 - val_accuracy: 0.5541\n",
      "Epoch 108/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0240 - accuracy: 0.6279\n",
      "Epoch 108: saving model to ./emotion_detector_models\\model_108.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 1.0240 - accuracy: 0.6279 - val_loss: 1.2384 - val_accuracy: 0.5438\n",
      "Epoch 109/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0258 - accuracy: 0.6286\n",
      "Epoch 109: saving model to ./emotion_detector_models\\model_109.hdf5\n",
      "56/56 [==============================] - 31s 545ms/step - loss: 1.0258 - accuracy: 0.6286 - val_loss: 1.2854 - val_accuracy: 0.5243\n",
      "Epoch 110/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0251 - accuracy: 0.6255\n",
      "Epoch 110: saving model to ./emotion_detector_models\\model_110.hdf5\n",
      "56/56 [==============================] - 31s 550ms/step - loss: 1.0251 - accuracy: 0.6255 - val_loss: 1.2089 - val_accuracy: 0.5444\n",
      "Epoch 111/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0187 - accuracy: 0.6295\n",
      "Epoch 111: saving model to ./emotion_detector_models\\model_111.hdf5\n",
      "56/56 [==============================] - 31s 547ms/step - loss: 1.0187 - accuracy: 0.6295 - val_loss: 1.2268 - val_accuracy: 0.5449\n",
      "Epoch 112/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0231 - accuracy: 0.6290\n",
      "Epoch 112: saving model to ./emotion_detector_models\\model_112.hdf5\n",
      "56/56 [==============================] - 31s 553ms/step - loss: 1.0231 - accuracy: 0.6290 - val_loss: 1.2498 - val_accuracy: 0.5452\n",
      "Epoch 113/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0208 - accuracy: 0.6304\n",
      "Epoch 113: saving model to ./emotion_detector_models\\model_113.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.0208 - accuracy: 0.6304 - val_loss: 1.2048 - val_accuracy: 0.5536\n",
      "Epoch 114/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0187 - accuracy: 0.6292\n",
      "Epoch 114: saving model to ./emotion_detector_models\\model_114.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.0187 - accuracy: 0.6292 - val_loss: 1.2730 - val_accuracy: 0.5254\n",
      "Epoch 115/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0227 - accuracy: 0.6276\n",
      "Epoch 115: saving model to ./emotion_detector_models\\model_115.hdf5\n",
      "56/56 [==============================] - 31s 553ms/step - loss: 1.0227 - accuracy: 0.6276 - val_loss: 1.2956 - val_accuracy: 0.5179\n",
      "Epoch 116/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0190 - accuracy: 0.6280\n",
      "Epoch 116: saving model to ./emotion_detector_models\\model_116.hdf5\n",
      "56/56 [==============================] - 32s 563ms/step - loss: 1.0190 - accuracy: 0.6280 - val_loss: 1.1971 - val_accuracy: 0.5483\n",
      "Epoch 117/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0080 - accuracy: 0.6338\n",
      "Epoch 117: saving model to ./emotion_detector_models\\model_117.hdf5\n",
      "56/56 [==============================] - 35s 617ms/step - loss: 1.0080 - accuracy: 0.6338 - val_loss: 1.2040 - val_accuracy: 0.5513\n",
      "Epoch 118/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0177 - accuracy: 0.6302\n",
      "Epoch 118: saving model to ./emotion_detector_models\\model_118.hdf5\n",
      "56/56 [==============================] - 32s 569ms/step - loss: 1.0177 - accuracy: 0.6302 - val_loss: 1.3175 - val_accuracy: 0.5112\n",
      "Epoch 119/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0188 - accuracy: 0.6291\n",
      "Epoch 119: saving model to ./emotion_detector_models\\model_119.hdf5\n",
      "56/56 [==============================] - 32s 564ms/step - loss: 1.0188 - accuracy: 0.6291 - val_loss: 1.2337 - val_accuracy: 0.5430\n",
      "Epoch 120/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0135 - accuracy: 0.6291\n",
      "Epoch 120: saving model to ./emotion_detector_models\\model_120.hdf5\n",
      "56/56 [==============================] - 32s 558ms/step - loss: 1.0135 - accuracy: 0.6291 - val_loss: 1.2541 - val_accuracy: 0.5396\n",
      "Epoch 121/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0116 - accuracy: 0.6293\n",
      "Epoch 121: saving model to ./emotion_detector_models\\model_121.hdf5\n",
      "56/56 [==============================] - 32s 558ms/step - loss: 1.0116 - accuracy: 0.6293 - val_loss: 1.2552 - val_accuracy: 0.5282\n",
      "Epoch 122/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0178 - accuracy: 0.6309\n",
      "Epoch 122: saving model to ./emotion_detector_models\\model_122.hdf5\n",
      "56/56 [==============================] - 32s 560ms/step - loss: 1.0178 - accuracy: 0.6309 - val_loss: 1.2379 - val_accuracy: 0.5352\n",
      "Epoch 123/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0150 - accuracy: 0.6298\n",
      "Epoch 123: saving model to ./emotion_detector_models\\model_123.hdf5\n",
      "56/56 [==============================] - 32s 559ms/step - loss: 1.0150 - accuracy: 0.6298 - val_loss: 1.1954 - val_accuracy: 0.5508\n",
      "Epoch 124/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0128 - accuracy: 0.6311\n",
      "Epoch 124: saving model to ./emotion_detector_models\\model_124.hdf5\n",
      "56/56 [==============================] - 31s 550ms/step - loss: 1.0128 - accuracy: 0.6311 - val_loss: 1.2418 - val_accuracy: 0.5379\n",
      "Epoch 125/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0076 - accuracy: 0.6321\n",
      "Epoch 125: saving model to ./emotion_detector_models\\model_125.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 1.0076 - accuracy: 0.6321 - val_loss: 1.2400 - val_accuracy: 0.5352\n",
      "Epoch 126/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0117 - accuracy: 0.6284\n",
      "Epoch 126: saving model to ./emotion_detector_models\\model_126.hdf5\n",
      "56/56 [==============================] - 31s 554ms/step - loss: 1.0117 - accuracy: 0.6284 - val_loss: 1.1981 - val_accuracy: 0.5432\n",
      "Epoch 127/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0076 - accuracy: 0.6330\n",
      "Epoch 127: saving model to ./emotion_detector_models\\model_127.hdf5\n",
      "56/56 [==============================] - 31s 554ms/step - loss: 1.0076 - accuracy: 0.6330 - val_loss: 1.2743 - val_accuracy: 0.5290\n",
      "Epoch 128/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0104 - accuracy: 0.6333\n",
      "Epoch 128: saving model to ./emotion_detector_models\\model_128.hdf5\n",
      "56/56 [==============================] - 32s 564ms/step - loss: 1.0104 - accuracy: 0.6333 - val_loss: 1.2009 - val_accuracy: 0.5525\n",
      "Epoch 129/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0062 - accuracy: 0.6338\n",
      "Epoch 129: saving model to ./emotion_detector_models\\model_129.hdf5\n",
      "56/56 [==============================] - 32s 566ms/step - loss: 1.0062 - accuracy: 0.6338 - val_loss: 1.2717 - val_accuracy: 0.5391\n",
      "Epoch 130/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0100 - accuracy: 0.6332\n",
      "Epoch 130: saving model to ./emotion_detector_models\\model_130.hdf5\n",
      "56/56 [==============================] - 32s 559ms/step - loss: 1.0100 - accuracy: 0.6332 - val_loss: 1.2329 - val_accuracy: 0.5371\n",
      "Epoch 131/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0021 - accuracy: 0.6357\n",
      "Epoch 131: saving model to ./emotion_detector_models\\model_131.hdf5\n",
      "56/56 [==============================] - 32s 568ms/step - loss: 1.0021 - accuracy: 0.6357 - val_loss: 1.2772 - val_accuracy: 0.5296\n",
      "Epoch 132/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0069 - accuracy: 0.6291\n",
      "Epoch 132: saving model to ./emotion_detector_models\\model_132.hdf5\n",
      "56/56 [==============================] - 31s 553ms/step - loss: 1.0069 - accuracy: 0.6291 - val_loss: 1.2053 - val_accuracy: 0.5483\n",
      "Epoch 133/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0022 - accuracy: 0.6317\n",
      "Epoch 133: saving model to ./emotion_detector_models\\model_133.hdf5\n",
      "56/56 [==============================] - 32s 556ms/step - loss: 1.0022 - accuracy: 0.6317 - val_loss: 1.2418 - val_accuracy: 0.5343\n",
      "Epoch 134/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0020 - accuracy: 0.6377\n",
      "Epoch 134: saving model to ./emotion_detector_models\\model_134.hdf5\n",
      "56/56 [==============================] - 32s 558ms/step - loss: 1.0020 - accuracy: 0.6377 - val_loss: 1.2017 - val_accuracy: 0.5642\n",
      "Epoch 135/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9998 - accuracy: 0.6365\n",
      "Epoch 135: saving model to ./emotion_detector_models\\model_135.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 0.9998 - accuracy: 0.6365 - val_loss: 1.2286 - val_accuracy: 0.5421\n",
      "Epoch 136/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9997 - accuracy: 0.6341\n",
      "Epoch 136: saving model to ./emotion_detector_models\\model_136.hdf5\n",
      "56/56 [==============================] - 33s 579ms/step - loss: 0.9997 - accuracy: 0.6341 - val_loss: 1.2241 - val_accuracy: 0.5416\n",
      "Epoch 137/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9990 - accuracy: 0.6352\n",
      "Epoch 137: saving model to ./emotion_detector_models\\model_137.hdf5\n",
      "56/56 [==============================] - 32s 557ms/step - loss: 0.9990 - accuracy: 0.6352 - val_loss: 1.1953 - val_accuracy: 0.5636\n",
      "Epoch 138/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9999 - accuracy: 0.6363\n",
      "Epoch 138: saving model to ./emotion_detector_models\\model_138.hdf5\n",
      "56/56 [==============================] - 32s 559ms/step - loss: 0.9999 - accuracy: 0.6363 - val_loss: 1.2261 - val_accuracy: 0.5466\n",
      "Epoch 139/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9937 - accuracy: 0.6378\n",
      "Epoch 139: saving model to ./emotion_detector_models\\model_139.hdf5\n",
      "56/56 [==============================] - 32s 556ms/step - loss: 0.9937 - accuracy: 0.6378 - val_loss: 1.2372 - val_accuracy: 0.5354\n",
      "Epoch 140/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9942 - accuracy: 0.6406\n",
      "Epoch 140: saving model to ./emotion_detector_models\\model_140.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 0.9942 - accuracy: 0.6406 - val_loss: 1.2364 - val_accuracy: 0.5371\n",
      "Epoch 141/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9957 - accuracy: 0.6364\n",
      "Epoch 141: saving model to ./emotion_detector_models\\model_141.hdf5\n",
      "56/56 [==============================] - 32s 566ms/step - loss: 0.9957 - accuracy: 0.6364 - val_loss: 1.2501 - val_accuracy: 0.5315\n",
      "Epoch 142/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9984 - accuracy: 0.6371\n",
      "Epoch 142: saving model to ./emotion_detector_models\\model_142.hdf5\n",
      "56/56 [==============================] - 32s 568ms/step - loss: 0.9984 - accuracy: 0.6371 - val_loss: 1.2130 - val_accuracy: 0.5385\n",
      "Epoch 143/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0000 - accuracy: 0.6352\n",
      "Epoch 143: saving model to ./emotion_detector_models\\model_143.hdf5\n",
      "56/56 [==============================] - 32s 562ms/step - loss: 1.0000 - accuracy: 0.6352 - val_loss: 1.2159 - val_accuracy: 0.5463\n",
      "Epoch 144/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.0028 - accuracy: 0.6327\n",
      "Epoch 144: saving model to ./emotion_detector_models\\model_144.hdf5\n",
      "56/56 [==============================] - 33s 579ms/step - loss: 1.0028 - accuracy: 0.6327 - val_loss: 1.2809 - val_accuracy: 0.5190\n",
      "Epoch 145/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9978 - accuracy: 0.6358\n",
      "Epoch 145: saving model to ./emotion_detector_models\\model_145.hdf5\n",
      "56/56 [==============================] - 34s 606ms/step - loss: 0.9978 - accuracy: 0.6358 - val_loss: 1.1966 - val_accuracy: 0.5511\n",
      "Epoch 146/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9971 - accuracy: 0.6349\n",
      "Epoch 146: saving model to ./emotion_detector_models\\model_146.hdf5\n",
      "56/56 [==============================] - 35s 611ms/step - loss: 0.9971 - accuracy: 0.6349 - val_loss: 1.1926 - val_accuracy: 0.5667\n",
      "Epoch 147/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9989 - accuracy: 0.6361\n",
      "Epoch 147: saving model to ./emotion_detector_models\\model_147.hdf5\n",
      "56/56 [==============================] - 36s 647ms/step - loss: 0.9989 - accuracy: 0.6361 - val_loss: 1.2527 - val_accuracy: 0.5402\n",
      "Epoch 148/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9954 - accuracy: 0.6345\n",
      "Epoch 148: saving model to ./emotion_detector_models\\model_148.hdf5\n",
      "56/56 [==============================] - 32s 576ms/step - loss: 0.9954 - accuracy: 0.6345 - val_loss: 1.2465 - val_accuracy: 0.5338\n",
      "Epoch 149/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9936 - accuracy: 0.6367\n",
      "Epoch 149: saving model to ./emotion_detector_models\\model_149.hdf5\n",
      "56/56 [==============================] - 32s 567ms/step - loss: 0.9936 - accuracy: 0.6367 - val_loss: 1.2325 - val_accuracy: 0.5385\n",
      "Epoch 150/150\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.9926 - accuracy: 0.6399\n",
      "Epoch 150: saving model to ./emotion_detector_models\\model_150.hdf5\n",
      "56/56 [==============================] - 32s 561ms/step - loss: 0.9926 - accuracy: 0.6399 - val_loss: 1.2220 - val_accuracy: 0.5477\n"
     ]
    }
   ],
   "source": [
    "model_info = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=150,\n",
    "            callbacks = callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b688e902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABl9klEQVR4nO2dd3hcxdm370fSqvfibuGKe8GYDqaDqaa3kEpiCEkghbyBNL4kb9obQoAktCQOhICB0AIBgukGbMAF27g3XGTZli1bvUvz/TFn9pxd7Uq7klYrW3Nfl67VnnN2z5zdPfObp8wzopTCYrFYLJZgEuLdAIvFYrH0TaxAWCwWiyUkViAsFovFEhIrEBaLxWIJiRUIi8VisYTECoTFYrFYQmIFwmLpAUTkERH53wiP3SYiZ3X3fSyWWGMFwmKxWCwhsQJhsVgslpBYgbD0GxzXzvdFZJWI1IrI30RkoIi8KiLVIvKGiOR5jr9YRNaISIWIvCMiEzz7jhKR5c7rngJSg851oYiscF67SESmdrHNXxORzSJyQEReFJEhznYRkT+ISJmIVDrXNNnZd76IrHXatktEbuvSB2bp91iBsPQ3LgfOBo4ELgJeBX4IFKLvh1sARORIYD7wbaAIeAV4SUSSRSQZeAF4DMgH/uW8L85rZwDzgBuBAuAh4EURSYmmoSJyBvBr4CpgMLAdeNLZfQ4wy7mOXOBqoNzZ9zfgRqVUFjAZeCua81osBisQlv7GH5VSe5VSu4D3gI+UUp8opRqB54GjnOOuBl5WSr2ulGoG7gLSgBOB4wEfcI9Sqlkp9QywxHOOrwEPKaU+Ukq1KqUeBRqd10XD54B5SqnlTvvuAE4QkRFAM5AFjAdEKbVOKbXbeV0zMFFEspVSB5VSy6M8r8UCWIGw9D/2ev6vD/E80/l/CHrEDoBSqg3YCQx19u1SgZUut3v+PwL4nuNeqhCRCmC487poCG5DDdpKGKqUegv4E/BnYK+IPCwi2c6hlwPnA9tF5F0ROSHK81osgBUIiyUcpeiOHtA+f3QnvwvYDQx1thmKPf/vBH6plMr1/KUrpeZ3sw0ZaJfVLgCl1H1KqaOBSWhX0/ed7UuUUnOAAWhX2NNRntdiAaxAWCzheBq4QETOFBEf8D20m2gRsBhoAW4RkSQRuQw41vPavwA3ichxTjA5Q0QuEJGsKNvwBPBlEZnuxC9+hXaJbRORY5z39wG1QAPQ6sRIPiciOY5rrApo7cbnYOnHWIGwWEKglNoAXA/8EdiPDmhfpJRqUko1AZcBXwIOouMVz3leuxQdh/iTs3+zc2y0bXgT+AnwLNpqGQ1c4+zORgvRQbQbqhwdJwH4PLBNRKqAm5zrsFiiRuyCQRaLxWIJhbUgLBaLxRISKxAWi8ViCYkVCIvFYrGExAqExWKxWEKSFO8G9CSFhYVqxIgR8W6GxWKxHDIsW7Zsv1KqKNS+mAmEiMwDLgTKlFKTQ+yfA/wCaEPnlH9bKfW+s28bUI3O325RSs2M5JwjRoxg6dKlPXMBFovF0g8Qke3h9sXSxfQIMLuD/W8C05RS04GvAH8N2n+6Ump6pOJgsVgslp4lZgKhlFoIHOhgf42nlk0GYCdkWCwWSx8irkFqEblURNYDL6OtCIMCFojIMhGZG5/WWSwWS/8mrkFqpdTzwPMiMgsdjzDr9J6klCoVkQHA6yKy3rFI2uEIyFyA4uLidvubm5spKSmhoaEhJtfQV0hNTWXYsGH4fL54N8VisRwm9IksJqXUQhEZLSKFSqn9SqlSZ3uZiDyPLoQWUiCUUg8DDwPMnDmznZuqpKSErKwsRowYQWDxzcMHpRTl5eWUlJQwcuTIeDfHYrEcJsTNxSQiY0y5ZGcFrmSg3Kl8meVsz0CvnLW6q+dpaGigoKDgsBUHABGhoKDgsLeSLBZL7xLLNNf5wGlAoYiUAHeiV+FCKfUgelGTL4hIM3qhlquVUkpEBqLdTqZ9Tyil/tvNtnTn5YcE/eEaLRZL7xIzgVBKXdvJ/t8Cvw2xfSswLVbtCkn1HvClQ2p258daLBZLP8GW2gCo2QuNVTF564qKCu6///6oX3f++edTUVHR8w2yWCyWCLECASAJoNpi8tbhBKK1teNFvl555RVyc3Nj0iaLxWKJhD6RxRR3JDFmAnH77bezZcsWpk+fjs/nIzMzk8GDB7NixQrWrl3LJZdcws6dO2loaODWW29l7lw97cOUDampqeG8887j5JNPZtGiRQwdOpR///vfpKWlxaS9FovFYuhXAvGzl9awtjSEK6m5TlsRSbujfs+JQ7K586JJYff/5je/YfXq1axYsYJ33nmHCy64gNWrV/vTUefNm0d+fj719fUcc8wxXH755RQUFAS8x6ZNm5g/fz5/+ctfuOqqq3j22We5/nq7iqTFYokt/UogwiPQS0uvHnvssQFzFe677z6ef/55AHbu3MmmTZvaCcTIkSOZPn06AEcffTTbtm3rlbZaLJb+Tb8SiLAj/fLN0NYKReNi3oaMjAz//++88w5vvPEGixcvJj09ndNOOy3kXIaUlBT//4mJidTX18e8nRaLxWKD1BDTIHVWVhbV1dUh91VWVpKXl0d6ejrr16/nww8/jEkbLBaLpSv0KwsiLDEMUhcUFHDSSScxefJk0tLSGDhwoH/f7NmzefDBB5k6dSrjxo3j+OOP7/kG7F6l53gUjun597ZYLIc1onrJ994bzJw5UwUvGLRu3TomTJjQ8QsrdkJDBQyaErvGdRfzPXUwYzrktT58GmQNgWufiF3bLBbLIYuILAu37o51MYF2MbV1PC8hrrS1wp5PoTG0q6pDGqqgPuyyHBaLxRIWKxAACQmA6rVMpqhpbQbVCi1dKMbXXN81YbFYLP0eKxCgYxAQszhEt1GOddMVAWup11aExWKxRIkVCNAuJnA74r6GEa6uCFhzfczqTFksceeTf8K+DfFuxWGLFQjwCEQftSBMfCTa9rW1abdUY3XfdZ9ZLN3hP9+BZY/GuxWHLVYgwBWItj4qEH7LJsr2mZiFatWWhMVyONHWCq1N0Fwb75YctliBgJhaEF0t9w1wzz33UFdX5wpXKCugtSm8sHlFwbqZLIcbLY36sakuvu04jLECAZAQuyB1jwiECuNiUgrK1kPtvtBv0OIViCgzmdraoDmGS5i2tcGjF8Haf8fuHJbDG2MhN1uBiBV2JjXENEjtLfd99tlnM2DAAJ5++mkaGxu59NJL+dnPfkZtbS1XXXUVJSUltLa28pOf/IS9e/dSWlrK6aefTmFeFm8/+ef2AtHSqNvc1hz65N2xIJbNg4V3wXfXdTg5r8sc2AqfLYSCsTBxTs+/v+Xwx29BWBdTrOhfAvHq7XrCWTCqTfsxk1IhwRfdew6aAuf9Juxub7nvBQsW8Mwzz/Dxxx+jlOLiiy9m4cKF7Nu3jyFDhvDyyy8DukZTTk4Od999N2+//TaFSfVQtz+EQJgYQ5gAdHM3LIj9m6B6tx6dJWd0fny07F6hH6tKe/69Lf0Da0HEHOtiAvAPkGOb6bNgwQIWLFjAUUcdxYwZM1i/fj2bNm1iypQpvPHGG/zgBz/gvffeIycnJ/CF4eZB+AUikhhElALRUOm8ribC46vg5dsiP770E/1YtSu6dvUmDVXw5+Nh17J4t8QSChuDiDkxsyBEZB5wIVCmlJocYv8c4Bfo1JwW4NtKqfedfbOBe4FE4K9KqfBD9GgIN9Jva4U9q3TNoqyBoY/pAZRS3HHHHdx4443t9i1btoxXXnmFO+64g3POOYef/vSnnvaFmQfRmUB4YxDRTpYzAtFUA0TwmWx9B5b8BcZfAKNP7/z43Sv1Y3X0izT1GhU7YN86+Ow9GHp0vFtjCcZvQVgXU6yIpQXxCDC7g/1vAtOUUtOBrwB/BRCRRODPwHnAROBaEZkYw3bGNAbhLfd97rnnMm/ePGpq9Ch7165dlJWVUVpaSnp6Otdffz233XYby5cvD3xtuCB1cwgX05K/wrb3nf3dsCDqK6J73cHP9GMk5UDa2hyBEB1gNyPBnubAVvj4L11/vbn2g9t6pDmWHsZaEDEnZgKhlFoIhK0Sp5SqUW4p2Qxc/86xwGal1FalVBPwJBDbKKZIzNaE8Jb7fv3117nuuus44YQTmDJlCldccQXV1dV8+umnHHvssUyfPp1f/vKX/PjHPwZg7ty5nHfeeZx+ibO8qLd9Snk6Y8/2d34Lyx/T//eEi6kpQpfRga3OOYNu1tYW+M93AzvZg5/poPnwY/XzWFkRK5+CV27rehAzHgKx9O/w0Km9d75DGRuDiDlxDVKLyKXAr4EBwAXO5qHATs9hJcBxHbzHXGAuQHFxcTcaE7tFg554IrDU9q233hrwfPTo0Zx77rntXvetb32Lb33rW7B3LbQ2BloKrU34NdW7vaXB7dS7k8XkF4gIO9cDjgURnBpbsR2W/g0Kj4Tjb9LbTPxh3Hmw8yMdqM4bEV37IsFcc0NV1wLt5vW9KRC7V+i/1mZIjDJhor9hBKKpVt8Dsci26+fENUitlHpeKTUeuAQdjwBPyNh7aAfv8bBSaqZSamZRUVHXGyOJfX8mtVfAzM0RvNhRc5078jUxCEnogkBU6MeoXUxBM7bN6K5mj7tt9wpITIbRZ+rnscpkMnEXI3bRYq69cqe2hHoD49rrapsPVxprOkjScGZUW3qcPpHF5LijRotIIdpiGO7ZPQyIfS5kDC2IbhMqSG1uDl+ap5ifgrYWd9RvLIj0wuhcTK0trhUSiYuppQkqS5xzBlkQ5nm1RyBKV8DASa7VECwQq56GHT2w/GqjycTq4ixy85m1tfRetpURBiMUFqg7AL8fB+teCtzujV3ZuRAxIW4CISJjRLRNKCIzgGSgHFgCjBWRkSKSDFwDvNidc0W0ap4kuCP1iu1QU9adU/YcSqFjDELAmhXNDXrORoIPlHKu0dnndzE5nXPmgOgEwtuhRpK2WrHDFalwFoQRCKX0MqiDp0NqNiRntReI134EHz0YeXvDYa65q+XOvZ9Zb7mZjOVWf7B3zncoULZO/6YrdgRu9yZE2DhETIhlmut84DSgUERKgDsBH4BS6kHgcuALItIM1ANXO0HrFhH5JvAaOs11nlJqTVfbkZqaSnl5OQUFBUhHPsoEZ1U5pfToTaogoyj+fk0jWglJesa0atNupZYGSEoBEVRbK+Xl5aQmOyVDTKfeXKdfl5YXnUCYTgoisyCMewlCWBCOYBiBqNmrR/YDnMS07CGBo/PWFp3ZFOl8io4wwtDYTRcTOALRC8Fjv4upIvbnOlTY75TzDh58BFgQViBiQcwEQil1bSf7fwv8Nsy+V4BXeqIdw4YNo6SkhH37wtQrMtTu1x3wvhaodDqzsmbtwokWpXRHbmo8dYe2Fqgq0z771iY4uE6/b2WJG3htqiF1YCbDclOc507H1lwPvnRIzYluBOx1b0QiLAc8AhGc5hocgzCjwLwj9GP24EALom4/oCLPnuoIvwXRVYGogqzBWrB63YKoiO51TbXwzq/htDtiM/M9nuzbqB+DBx8BFoR1McWCw77Uhs/nY+TIkZ0f+PxNsO0DuO5JePoqvW3y5XDFPNj5MSRnwsCg6RiL/wx718AlQcX43vpf+OghXccoJbN7F7B3DfzrKhhzFmx+A769WlsEvz4ezv65doUt/Tv8qNRdOMVkdbTU6/IhKVnRuVm8HWqkFoQvXYtpsKlvLIj6g/oGNwKR62ScZQ+FLW+7xxtLoycsCG8WU1dfn5qrr6s3BEIpTwwiShfTtg9g0R9hxCw48pyeb1s8MRZEcMl6r0BYCyIm9IkgdZ8gOUN3hpWOu2PwNFj/MnzyOMybDS9+q/1rtr4LK56A2vLA7dsX685l8+vdb5cZBWcM0I9mASCAlGwtAMH54G0t2vxurtedW0pWdIFar0BE0lEf+EwHnH3pIVxMnhu3Zq8rEDlOHkL2EL3dZAnV7NWPTVHO2wiF38XUjRhESpa+tt4QiMZqN5YTrYvJWGiHgmuqtQW2L4r8eGNBdORisjGImGAFwuBL1z8y4w8/5Tbd8f77Zh3A3r2y/QimsQpQsOUtd1tbm1uIridKWZsOOtNJ4fUuIZqSpQVCteq8eW/n3FQTJBBRrCpnOpm0/MgsiANbIX+UI1bBQWrPcyMQafmuZZU9RLe/tsw9xnvdXaW12W1Ld4LUXoFoqIRHLtRlRWKBt3OP1sVU7Xxuh0Jwe/1L8PfzAl2T4WisgaowGXIBFoR1McUCKxCG5Ez9g6vYoQVh3Pk602bwNLj4jzo+Uboi8DVmpO21FMo36041LR82Luj+Sm5GDMJZEL5Ud3vw+g9+gciOblU5c105wzrvqNvadOeZN0K3JVyQGvSM6cqdrnsJtIsJ3DiE6eiinfkdjPf13ZkHYQSi/gC88n3Y9h6seb57bQuHt53RdvTGgjgUBKLKmTlfH7bQgkv5Jvf/YCvBWhAxxwqEwQT29m/UgcnEJPjKf+Fr72j/P0DJx4GvMSPTzW+6cxXMLOFZt+nA2eY3u9cu09FlOgLRXNfeggDdMXs746YaLRq+dH0cRO5qaajUmVJZgzq3IKp361ne+SMhKS18mivozr9iB+R6prlkD9GPxnIzHV1ro7YCukpAqm4PWBAAq54CBHYt73q7OsJrNUTrKqo+hATCCEMko37jXvKlh0iAqNe/00jf61Dh7V/puUB9ACsQhuR0/bh/k9tp+dJ0+mtmkXah7AwSiMYqPbKv2w+7HWEo/UR3lDNv0MHkdd2awuGJQRgXk9eC8AhES7BA1OrOOSlVWxDe9+qMhkqd+ZSc2blAmBpMeSOdIHWImzglW6fbVpdCxU7IPcLdn2UEwrEgjIspmvaGvIaq0P9HQ2O1brsRiKzBcMxXoWxt56vt1e7vQv2rCv2YmhO9i8l8bt2ZYLfpdbdDjiV1Tswukk59/wYtAkXjQwSpG/U9BoeXBfHJ493vN3oIKxCGZMcnfmCL6/bwMvw4XTfI+PHb2nQHMPFiQPTNBVogBk/V7pbxF8CGV7tXrTRYIFrqwwuEd4TVWKM7MRODgMhH0vUVupNKyezcxWQspoGTw2QxOQsOZQ7UGVkt9W6AGiA9X1+DmYld7RGI7qS6mmtNjiBAX7ETHjwlcLZ3W6s+f0qWriM1dCZcdB+MnKWTAMzCU+tfbj+BC+Afl8BrP4yuzaZzzxsRvSXQEzGIF74Oz94Qeayqq9RFY0Fs0IOz1OzQWUzp+fr/7rpyO6NiB7z5i9h/NuC6h/sAViAMxsXU1qJ978EMOyYwH94EqPNH6bUC1v9Hl5zYswqGzNDHTJijj9v6rn6+cwn884roBKOpRouXsXCCLQhfOAuiWnfOvjR9c0F0FkRaru5cO+ukd36kP4OsgYEZVQYTB8kc6Fpg3hiECBSOhX3r9fOaPa7odSdQba41Z1jnMYi9a/T3tme1u81cd0qWbv/X3tTpo2ZdiF3LdKfx5HU6pdlLUy3sXe1mxEWKaWfeiOhcTEp1PwahlH7tnlWw8bWuvUek+C2ICL7f/ZugaFxo92VLo3Y9JaXG3sW09kV47y53IBMr2tp0n9FH0natQBh86e7/xsXkZbhTUNZ0cv44QDbM/IoeUT73Nd0pDzlK7xt1KqTkuNlM792lA9oVnmK1nRUIbKzSAmHa5y3GF2BBNAZVbzUxCI8FEamrxbiYUhwXU7g2KqVrJg0/Xj8P52Lypet4hun0vAIBMGCSrlirlJ7XkT/auYYecDHlDOv8uo3V4+1czbmNuBqyh2ixK10OK5/U24LLspStB1TXXEySoC2saFxF9QfdYnVdTXNtrteDI4B3fxvbkbKxIDobALQ2a4u+cGzoBIiWBv37NxmI3eGjh3WGWtg279ePPTGBsyOaawHVZyb+WYEwJHsmtIVyMQ2YoEfUOz/Sz02nk5oN06+D6dfD2hf0NiMQSSm6pPX6/+iUvk0L9HYTpCtZBr89IvQ62QYTKPW6khqr9Igq0ecJUte3T/trrtPH+V1MkVoQFU4MwrGqwv1Yy7foG6fYEYhQaa4tjgWRNcjd5g1Sg/5sq0t1DayWBigY5VxDNwTCCHjOUP1/R0Lsn8znyarxirAXEW0h7loGKx7X22qDZumXrQlsQ6QY115anv7cOotzGEz8ISWn6xaEaevg6Vr8tnQzuaIjIg1SV5Zo0cof7YhACAsiKcWZw9RNgdizEkqWus8PbNUTUA3RxE26g/ndWQuij+EtTxBKIBISdWxhr+OGMO6A1BzdaVzwey0MqblQMMZ93cSLdYf7ws3uJCjzYytbo2/Mt34Zvl1GIEzJj+Z6dxsEWRB1+KulN1V7YhBdCVLnuqJpSi0H3xw7nYqrxZ1ZEGmQ6QhEao7+8zJwkn40M6rN59ctF5PHguisdEdHFkSwQAAMnaHTmQ9ug8QUHZD2stcIRBcWaTICAZFbAyZ2MmC8voaujP7NgOe4G3W8a+VT0b9HJCgVuYvJfIdpeWEGH14Lopsdd1Otfn8zYXPFfPjPt93foJkM2930684w799Hgu5WIAzJHhdTTgiBAO1aMKNFr4sJtAn8hX/DDQt05pNh9Bm6o92xSAc7wb1BzHttfDVw9OLFiEFisnY/mHkQxvWR5NRfMiNOc2xDlU4V9XXFgjAuJud1TTU69/+ucYGd9o7F+uYtGOt8BmHSXI2LCdq7l8At3LfVCMRY97xdpaFKfw5m/khHcQgzMq3zWhBB36+XoU6MKTkLJlzouh8MRiCiXge8QgtzWq5+Hqk1YCyIovFuuffWZl0V18w56AxzvemFesQeq1X+mmpdd1hno3HzW0vJDD348FsQ6d0fcfsLXDptMveKmcDZWy4mvwVhXUx9CzNalkQtBKHIKHJHEn4Xk2c0nJqjA2pefGlwpLNa3Mnf1Y9+gSjXLqD0Al2/KRSNTiaNiD62uV6f23TexrJoaXRrLyVnuqNan3FFpUXm8mh2sqFMmivoH+3uldoqqfX423d8pOMPRhCT0nQH5Z2/0BzkYsoJIRDZQ7R7xATzC0wMoptB6pQsT4C+g2v31ovyvh5CWxBDZgACk+Zowavd547alXIFoqk6ukWo6iu0OBgLItI4hN+CmOBex+5VsPhP8NEDkb2HP8U2W6d1B7vNoqGpVgeXQ2F+++a4jghIxnAy5IJXT0xKBV9G90fcpi3mN2fcmya+ZO6nnqgR1hHmd2otiD6GcTFlDQ5fhTWjUJeObmkKdDF1xgnfgGnXwZQrtEvC3CR1+/UEuJO/o0fP3iwag9ed5EsN4WJyLIjmeqczTtUjLtORm+B2uHpMVaVuhwbudaXluuUwmmrc7A0jjLX79SzXYs9qsCajyusr9gapIbQFIaILIZpOqieC1I1VevRvLICORvPRupjS8+H6Z+Csn+tBQ1uL2/aavdrHbq4zmjiKif2k5rrPI6FmrxZz4xqtP6hnrAN8+mxkItXgsZgyBnRvPZSXvwcPnBg6/dcb5+lsNG72J5sYnApcOS7Aguiui8kIQ5BQmM8hmsyr7mB+d61NvbeKYQdYgTAkpQIS3r0EeqQPumM3awyEckEEM/RouPQBPZJPLwh0MWUUwlin+mbZ2vavbfRYC0lprovJnDfJWBANbmecnOmOAE2MwtRjCub1O+Hxq9znfuHLdS2Iplp3prPZb1xiwz0C4Q2kG0yqbfYwbZ0Z6yAY42ZKStWfSWJKN4PUjhvOCHhEFkQEQWrDmLMgo8Cdn2IsSxOjMpldUa3DUdnexbTzY3j2ax138tV7tNXrjV0YQa8q0a7Azmj0WMSZA/Rn0ZWZ7Ps26BnnrU2w8K72+81vXxIisCA8FQO8MThDT2YxmbaY35wRgpq9+nMwYt1bAgF9IpPJCoRBxBmFhUhxNWQU6sfa/XrElZQKScnRnSc93/V11+7XPt+8EfqGKd8ceKxSQRZEWngLwkyUC+VigvACcWCr7kTM6NlrGfljF14Lwtlv8u69FkGom9i4mDIK9FyCoz4f+nMx7pHMgfq7iGSSXkc0OBaEEYgOYxAdWBDe7LZQmEGDEeS9jsibwH00cYhQLqbl/4BPn+64blHNXm2heYWlskQPHnzp8Om/Oj+3NyvP+zuPlnd+rc875Uqd5RVckK/O+YyzhkQfg4AQAtFDWUx+gQhhQQTEpmIsEN7fSx/IZLIC4WXqVTDh4vD7zWixbr8byI2W9HyPi6lcv2dSis62Kd8SeGxzvS6y53UxmTTXdjGIBrczTvFYEGZ/Wl7gD91gOn6zloTfF53rdo4NFW4pjOA1k01nBu0tCKXcVFvQWV7GDRWMyWQy8Z9Iynx0RDsXUwRB6mCBSM7sfNEnvwVhBGKNdlOaBZEitSCa63VSQWqujscg+nP3p1V30P5gC6L+oBb93GI9m3/tC9ot2hGNVXqQkpzpBvZro3Qz7VmtkxmO/zqc/QtdXmXh7wKPMb/93OERxiBExxj8lrLzXSnV81lM4IlBmOylssAkhF61IKxA9C0uvBsmXxZ+f7oZWZW7HVC0pBfojlopx8XkjEDzR+tJQV78Plino05KcyfKGYFISNI3tinWZywI46s1ApFR1D7bpqXRtQTK1ulHrwVh4jLlW9ylT70L2iT4AicYBo/yjFBEsiqfsSCyHIFIyep+mmvUQeoKXWLD+/rOCBaIsjVa7FIiOK8XI7ipOTron5qtrbv9GwP3h8JvQXgsj8oSPeiYcpX+rjorUd5Q6SZDmMKQ0QSqG6vhhZt0+0/8pl4p8Kjr9WRCrzjVlQOi4yWRxCCSM/Xn4f9tOb+pthadNu5L7X4Wk1IhYhCeILU3sN5bQWqwAnHI4Te992lTMHiWbSSYGERjte7EjegUjIbyrYFZGibQaG58X6ojLh6rwmQ3GReTiUEYkjwCEewy8JYNMKUuvAXjzPvsW+ceZwSioUK3y7tmt9eaAbfj9YpIONLydGn1wdP08+TMrldhBff7SUrVQhaJiwnPim4NEQqEPy5VruME+zfpdNNoBcKbHAD689j8hmd/RejXNdbozi1zoP6cE3yuiylnmPt5VoYIGAecv8qxXHBFryZCgWhtgX99WbvXLp/n/l4LxurfqlcI6g/oa0zNjiwGEWwpBw8+TBZTa6Mr7qbtkdYzaq4HnPuuXQyizL1vJKFnFrLqCK8FcTi7mERknoiUiUiI1BwQkc+JyCrnb5GITPPs2yYin4rIChEJM0EgDqTm6kBr3f7uWRD1B93sCHMz5o/WgW/vaGXVv3Qu/+gz9HNfujuq83ZeSSnuRDmTxWTwWxCF+kfv/dEZgZBEj0B4LIiEBH3zla13X+O1ILzuJXDFyHS45jHSdb3nvguzvu9eX1fNeW/sRkRfS4dZTJ6OxLiZvFZaRyQl6/ev3acFvaVBl4boyux1cDOYUnMDXV7hBMLMgcgapK81LU9vq92nBSKU7z4UjVWuyzQzShfT+3/QJWQu+D2MPcvd7lkz3U9dub4HkjMii0GY37LffWkEotHdbuYwNdXqmM2Dp8BdY+A/34ms/d52hIxBOPdkdgTro3SXfhSkfgSY3cH+z4BTlVJTgV8ADwftP10pNV0pNTNG7YuehAT9467tTgyiAFDuQigZHgsC3DhES5MOTo47361YmZTqEQiPOJkJas0NupP2WhBm9O6NnxiMhTL8OFcEasqc4KbJfsp03VBp+YExCDPa9Z/LpLl2wYKAQGskVJD6hZvh2a92/j7NdY6V5XxGqdmduJjqtKsOohcIcKyzfW7uf8FY17qMNEjtj+nkOo+O+PpTVytCv85MaDOxm7Q8N5MqGoHwWsTJmfq3Fmmq64ZXoPgEmPnlwO3+jtszKKk74AhEpv7Nekf9wQQkaHiKVYLHgkhx9zXVwn9/qL/PARNh2/uRtd8rYI01Omup1RGgmr2uBRFJ3KS7NFZrKxDaWxBL58G7v2v/mhgSM4FQSi0EwqZeKKUWKaXMEOlDIEQJ1T5IRqGbxdRVFxO4QWHz3OT+mzjExv/qkctR17uv9aW5BdW8AmEsiBZPkNr/GqfTDvaVg2NBCIw5Q4tA3QF93iNOdI8xYpOaq1OAI7EgzCjPLxARWhBegoPUrc266OGG/3bcqUBgRg7oz6ozC8KsS9Flgdjvin7hWG15IdHNXgfXgjBCYSZZhnORmeKRJoaTlueKfc4wnVqdkBSBBVHp/qZEdKA6khhEU62eROn9zRi8adKGugN6oOG3LoI63OBFr8x7+AcfTqcZYEGYxb42aBfQCd/Q9dEqd0bmJguwIGrc7yx7mBaKA1v195Ka27NB6v2b2g+CGqtdsQ+OQaz6F7z9v+GrLsSAvhKDuAF41fNcAQtEZJmIzI1Tm0KTXtBNF5NjDZjgo+m4847Qrh5jQax4XGfDGPcSuGY2BLmYnPTX5gYnaNeBBeGNQ1Ts1K6JQY53b+V8XV9o8uWe8zjvlTNc+6jNSNyUhfAS1oLogkAEB6lLP3FcZNXuZxcO75KsoIWisyym7FACEeH3awYN+zdpqzKjSFub4SYnhiKUiwlg5Kl6Tkg4F9PG13ScwUxETMvVy+OCW7Y+VKG7duevDBzwZBZFZkHsWqatteIT2u8L5WKqP+C6mCCwc96/CX49zF3aNyCdOyi+FcqCKFmiHwdMckvul0aw+l+wQJj2mqKRZev0d5yS2XO1mBpr4KFZerZ7wPZqN1EjWDzNb+m/t/fOuhT0AYEQkdPRAvEDz+aTlFIzgPOAb4jIrA5eP1dElorI0n37ulEeIFIyCrVZ31zXvoOMhDRHIIwFYVxMiT6dlnhgi15HYNPrMO3awDRLr6umXQyiwSm1EeRiMqLiDbAbKnfoTsSUB3n/DzrmMf4C95hk5zw5Qx1fvtfF1JkFYWIQEbqYvKRkBZaq+Gyhu890BOEIrqOUEoGLyUyQ9JeijsKCSC/Un2v5Ju1eMq6ylOzIOxRvFhO4A4ni4wM/dy91B/QyuEd6PLne78S4p0It5BRMQ9CAJ1ILYseHgOj1UoLxlqj3t7kc0vNCWxe7lmkL2QwATJkZcw3e9/IGqY0ry4ysB0zQoikJkS0P6w08N9a4AxNj1e/foL/j7qZee9mxWF+LWV/Gf/4qt7Bl8HfWUKW/35Ilkc1t6QHiKhAiMhX4KzBHKeWPziqlSp3HMuB54Nhw76GUelgpNVMpNbOoqCjWTdajQxPc7Y6Laf9G7Ybwjq4LxmgLYuH/6R/30V8KfK0vjAXhS3M7kHYuJk+QGtq7mHKG6z9fht439pzA2ILfghjmdlStLfqHHHEMoosuJnADdZ8t1KvWpeZGIRDOZ5SaG4GLabD+31RDjTTNFZwU4nIt+oVj3e0pWZ0vVmRoqNBinOjEQo76PFz8J3cCXKgYxOY3dKrn2HPdbUYgMga4kyhDLeTkxQT1vTG1SOsx7Vis/f3BvwVoLwJNdc4qcAWhrQsjDCYobNZCgfZprn4XU4rjzkP/LnKK9X2ZkgmF46KzIBKS9P+mTfmOBdHapO+f5IyeC1KbtGPvKoagr9lvQQQLRCVMvkKL33u/75l2dELcBEJEioHngM8rpTZ6tmeISJb5HzgHCJkJFRfSC92y3V3NYgL9QzCdtqFgtO5klj+mFyEyk60MSZ6ONjgGYVwjPo8FkZjsWiDJGY4IOC6mtjYtELnDtTvEWBHB80CSQwiEPyUzyILwjxh7woLwlBpvadQTxkbOgmEz9ToaHREcg+goSN3Wpi2e5ExnLeiDToehonAxFenja/YGCkRqFBZExQ49d8CQPxJmOLPOw1kQG1/T5zbrj4Br1XpXReysFEVTrXYTpQZbEPs7LvHR1qpXSTSzxoMJFgHT8Yd1MTndQO1+d25Cu7L2IdJcjQVRV65rehmGztAWRGfuGNOGjAGBMQhvWZj0fN2W1saulSAJ5jOnMKXXjWeEOi1fB6q9WUxmpbm0PC0S+9ZHXqm3G8QyzXU+sBgYJyIlInKDiNwkIjc5h/wUKADuD0pnHQi8LyIrgY+Bl5VS/41VO6PGTGyDrmUxJad75iYECUT+aP0DTEp10z29BFgQQXMdjECYiXLQfuSeUeiOCmvL9MjIrA89cJIWEK+7AtwbOWe4vt7GKjcTKtjFlujTcZQeCVJ7So2XLNEdwshZ2pVRtrbjjjfYgkjJ1u8TqpP1TuZLy9c+8s7qMAXj/U0UBFkQkcYgytbp+ROhSM1tH4NobdGppWPPCSwvb0TbW1PMlGgJR6jS5hlFWjQ6KvGxd412z4QViKAsJiMQafmhXUwmC6yuXH8vbS3t01z9gw9vDMKzlssAj0AMOUr/Vk22HugKxEv/HhQMd9qQOSAwBpFbrH/P4LqYwDmmVq9CF021XkNtuV4kTBLcDEFwsu/a9O8mePJfUw2gtIiPdDzu296L/txREssspmuVUoOVUj6l1DCl1N+UUg8qpR509n9VKZXnpLL601mVUluVUtOcv0lKqQ5W04kD6Z5OvSsuJnCtiIwgl5gZsZz4TW3iB2M62sQU130AjgVR4RyT7rmpggXC4zYwbjIjEGf8BL7yauDCSeB2ksaCAHfJ1GALwrSxq2muAef1lBr/bKG+mY44EYbOBFTHvuXgILXpLP9vFDx2WeCa4N42puVpoQ0WmM7wfo8BLqYILYjmBjj4mZuJFEwoC2LnR3rbkecGbvcLxHB3W6i1FLwEW1zg/v46ClTv+FA/hhMIX5CVYMQmlIuptcVN0Kgr99RhctqUkKBFot1EubTAtVxMyRZw1+3w/lbe+ZVeCOje6bDupcA2ZA0KjEGkZLlzQkyQGvT+9a/Aq9+H3Z+EvvaO2ObE00af4YihM9Pcu8xtcAlz8/2nZMOgKXrQ4I3LxYi4B6kPObydQVdcTOAGINODLIiRp8JF9+ry36EwHX5wx+VLc0th+DqyIDwCYUoxm+U/swa6s269BLuYQHdmENrv7F35K9qJcl68E80+W6iXwkzN8dz0YVL92trcsunmPaZfrxdzmny5XkrTJAgEt9EIhFkCNm9kZG01vwlJcP3W5vyRzIMo36RHjsFriRhCxSBWP6N/D94sN/AIhNfF1EmQ2t/5eCziSOoxlS7XAVWvGHlJStZuTr+LyQhEiDTXiu1u9lVduSvSwQkX/iwmbwzCIxBeC2LgZO2q8cYhasv19pQsWPATtw2SoO9Hbwwi2SMQwRaEmX/SlbLoW9/V722sdf8iZJ6BTXKQW7DRI+IJiTDiZCsQfRKvW6grLibwWBAFgdsTk3RgOlyH6p+8FiQQAdZEWvvJRQaTjgmu2e3tSEJxxIkw5mwdxPVbENv1Y6QWhDc9N1LMzVi7T2enjDxFP0/P126cnSEC1ZW74NELYeUTMPVqN/6SkACjToMTb9HPvVVzvW4wU2l38xv62oZMj6ytRiByjwj8LiKNQRjBKgpnQeTqTtz40luaYPVzejW74N9CpqcthkhdTAEWhNMxdjSPoK7cncEdDm/8w7hB00JkMRn3UtZg/b5NnlF8qPcKKLXh/M4TfIEWXFKKtsq8653UlevBxthz3I65qVa3x8ze93fUnsKFZnIfaAvCBJcjFYjGGvjTMTq1de2/YcRJbpaZcTMFlDcPcjEFL1A2cpa+D4OzoHoYKxDREksXU2d4F//x4u2AfWnu6Cy4cqqxIEyAOiWnc5EbdapeHCch0WNBbNOPodJ8gy2IpNRAH3mkmGvc9LoeWRq/K+ibPriwIcDrP9HzJebcD5c+1H6/Gd17q+Z6A+mm4u2Wt/TIvLNKroa0PEACOyfQI8GW+s6DmmXrOl4rIzUnsKbRpgU6JjH16vbHDpoK18wPjCX50tsvBesl1OJX/nkzHXSAoWbTB5Oc6YpAqEKQ5ppMgLr4eLdWGbSf9BkqiykpWWcgFR6p42Besga7QmDWw07PDyw901Sj25Oc4cYgEnz6vc2ktYyCwAW0TKceaTmSTQv0NUqithYnXeZmK1U75VICVtALqlAbbOWZ++Gz2MYhrEBES1qeNkeh511MnWGEILhTDxaIpJT2lVYhcPWz/ZvaZ0l1hl8gjAWR2/4Y701syo93BTNa2/SavvmHe/zc6fnt12turtdZPVOvgqM+F3pUm5yuZ8eGsyDS8vSM4pq9MPrMyNuakKgDoiNODtzuL9jXiRWxb70WB6/14cV8zqaTWPWU/i5Hnd7+WBEYf76bLguBvvtQhApSp+Xpz72jEbJZAa8jktNdEWiodNZQSXEy7JI8FsRGPVrPH61FusEzmvZfR1qIiXIeq9qbwWTwFqlsrtNJIOkFgaVnmmq1OKRk6s67dp8rBsYia+dicjr1SAsarntJn/Orb8Dt22Ha1e58B1NPy3vNwUHqYCuvaLx+vxi7mZI6P8QSgKnH1NwQ+QgzGL8FEaVAeBf/8eIVCP8Nk9netWNuiuo9ukTD9OuiO7/5cVZsd3L2fe2P8boBzAp3XcFcY/1BLQ7ekWRavjtfwQjBlrf0jTtxTsfvWzA6SCC8FkS+u31MFAIBMPft8NfQUOkOCkKxb334ADW4nXB9he7INv4XZt4QKAId0dlM6lBBapHQFYADXlfZ+WRR72I+3vplIoEF+/Zv0hZYRqG2lkwSRbLXxeSJpXgtCNBzRgqPbH9+k7lnrAcIFIjafa5AGAGo3uOed9ix2qWZOdCNkTRGaUE0N2gLYsoVgX2G340XxoLwzpEItvJEdMxy69s63birfVEnWAuiK6QXdt29BF0XCO9oyYsvyIIA/QMPleYK2sfeXBu6fk5HeFdnC+da8AYSzXKjXcGX5lpqJv5gSM/XlpA3hXTNC3rUOyLo2GAKxjhBYcefH2xBAAyc4pau6A6pEVgQLU6tn3DxB/CsUV2pXW6tTTD1ysjb0VkMoqFSuz6CxTxnmG5bOKJ1MQWXp/Hu279RC4S5N4wbM3hCqLdYX4LP7RgnXAhFYQSitUl/B36ByA8sPWNiEAEC4bjAxp8P31qq3Vje1Gt/DMJjQYRbQ3rr2/o1Ey4K3G6WIDbvFRCkDqp2G8rKm3CRFrhIixJ2ASsQXSGjsOvuJdCTvQZMcqfyR0qkMQiAmV+CSZcGHmduirX/1o/RCkSACyI3TBs9nVFLQ9cFQsS9Ib3xB3BH+iYrpqURNrwK4y8MbdV4KRijO0Tz2uA0V9DFC3sCfyZWB5lM+zvJYAKPMFfoOSAJPrd+ViT40vToN1wspNEpPBnslht2rM4ACrUanX8FvE5cTL4gF5P3eOPzry3XKbCFR4YQiKCqAN5y35EkP3gthQALwlNZwMQgUjwC4T2vwbvfu+IcwL6Nuo7Umufbv27ti/q6R4SoGJQ5KLwFEZzmmpgcOBgce45Oh13zXMefQTewAtEVjp2r5yp0lSFHwc2LordCwmYxhXAxnfK99rOizc2ya6kWp2hHyQmJngJ4uaGPaWdBdNHFBPqGTEzRHZUX464xefVb3tYTtiZe0vl7FozRj8bN5E1zLTpSjyIjeZ9IiCQGYdbh6MjF5F9rukILSsHoyN1L0HnJ74aq0B198fH6u9y9MsRrgqrPhiPZk88fUiBqda0j0K4cIxAV2/EvN2oIngcRLmbjxbu+tj/NNlggat0gNei4RKi1yJNStVVrEiRSst0Yze6VWrxeuDkwa6q1WZdDH3d+6PXrswZ6BKJKx1kSfe3X2Q71HSWnw7jztAD1xOzuEFiB6AoTLw4sw91bhJsHEcqCCIW5+UCn2XUF8yMNleJqzt8TQWrQQlB8XPtsLL8F4QSqNy3Q1kawpREKkylkbnKviylvBNxR4s616C7+9bA7sCD2rXcymMaEP8br2guu9xQJwSv9BROuMrGZALdjcft9wcUFw+F1lQR3csbFtHuVfj5oSqAFYZYb9V5HtNapSQSpCxIIb+mZRpPF5LmvQlkQxqo1WXCDpmirrqUp0CX25HWugB74TB8z6rTQ7cscFJjF5E1Rb65zXaHhvqPJl+mBkind0cNYgTiUSMvTI5EjgrJlfCEsiFAkJrmd6xHdFYjc0PuD01y7Y0Fc+jBc/Mf224MtiMoSXbso1AgtmNwjdPZMOwvCaWdHOf3REsl62LtXaXHoaDSckgOIHu0e/Cx0MLYjvCv9tbbA/SfoUachnAWR6WQVmRnTXoKXSA1HgEAElRQ3LqbdK3QQOHuwKxDeOkwGX1AWU0QWRJCLSRLcazUBbH8MwmOtJGe1fy/QwuEVCPPeB7fplNpLH9L/b3pd7zPzjbzzUrxkDtAWhHcVRHBmhyv3eoM/O8OYs7RwrA7h2uoBrEAcSiQmwbXz9ajaixGFpLTOOzhzw0QbfzBEZEF4ajF1x4IYOFGP6oMJjkHU7IncXZaYpGdI+wWiHpDIOpto6SwG0damS2YEf5/BJCToTmD3Ch2cj1YgvC6mhkodx/DOLm6oDB9TKz5BWxDBBe/861eE+R0YjEAoFd7FVPqJW3AwOd0TawsaxSd5LYhIYxAeV1JduZO+6wS2M5w1L4JjEKHO7W9zpjs/wS8QZVoU8ka4Zc/NZFJ/SZswE1KzBun4kCnx4rcgzDwR454LY0Ekpejy/OtfiombyQrE4YC5UYJdMaHIHKBLI+QWd+1c5gYP53vuSYEIh98n7whE9V43ZTASCka7o0Bj5fSk5WBIStUB5XAxiP0bdEfrneMRjrQct4pt1ALhWZfBdOzeeSQmSB2K4uP152xmOhuicTGpVqdMfGN7gajdrzOYBk93txu3UKhsvWhjEEkpumOtLXcEwpNunFEIVbt0+7xprhA6BgGucCSmuN9DjWPZ5Y3Q+9ML3FI2lSXaajHl5IMxE/Gq9zgl153vwdSXavZkgIX7rE+5Db76ZucJGl3ACsThgF8gInDnnPETmPOnrp+rMwsiyakL1drcfRdTOMys7roDOge8dp876SgSzLobbW2xEzHQotNRPSbj2w9X7M5Lao6exAddj0E0N7jWjLG+IPzoFNyV4oLjEJG6mMxI2NQuCk5zbazUWVzekuXGhRjcSfvSnWyslsgtCHBdSXXlgXG4jEJ30qc3zRU6sCCc68ka6FrjlTuhqtS1dnOL3YKWlSVaHMIlFRjLt2ZP4CqG/nW2vQH+MN9R4ZjofxMRYgXicMBYDpHcMMXHhQ+YRUJnMQj/okH1se18zWS5unItSNFkZBWM1nGS6tLuTeaLhJSs8BbEjo+c2cOjQu/3Yiy2rCGRV5k1BLiYHIEwFkRrs+6kvR2nl4LRekQfHIfwu5gisCBAd6AQaHl6ff7eulemLeGSMVrqI7cgwC0xU3cgSCCKtFVj2pKUrC0+CB+DMNszB7lW665lgAoSCMeCqCrpuN6ZsSDK1mkR9RfaDLIgGqoCiyn2ElYgDgf8FkSMOmMvnVoQRiCc1cNi1fmmO2s3mBTBaFxMJmBYsaN7k/kiIS03cE2FzW/C6mf1/zsWa+shEveW+dy7MlL0LtfpXTIWXEsiuHCkQUT72oPXAa+v0NZBZ26NdgKR035f1uBAgc8I52LyWEItDZFbEOmFeiBRfyBwRru3Fpppi7EcwlkQZnvWQDcTyqxwaKr/5gzXVoVSzqqNEQjEaz/S2VRTr3La47EgWlu0UHS1OGg3sAJxONCbAtHZPAjTBu8CRrEgzam8alIEo3ExmfhLZUlsrRzQ5ad3r3KDvG/cCc9+Vc/8rtgemXsJXIutowl14TAi3eJxMRnRMos/hbMgALKHuC4iQ0ez6b0kB7mYgtNcITD+4G1LOIFoqXdcTJFaEOFcTF6ByAz9GIzZbmIKmUWuePotiCP0Z129R1cY7kggUjK1gGUOhC+/6pZ4Ma655rrQ1XZ7CVuL6XAgKQoXU3fJGarz9sMF3UwbzMg0lhbE/g1uTZxoLAhTZtlvQcTQxTT0aFg5XwdDU3P0JCrVpkUCIhcII8jRBqgh0KozPm0j4N7ZxeHIGqw7O2/Nn0gK9UEICyK7/T5v/MHbluBO2ruqXDQWREaRa2kGxyCC25IcoQXhr/JapDOYktLc36AZgJQu1y6s7E5K6n/lNf179lo3fguiNnCxoF7GWhCHA75etCAmXAzfXOKWKm7XFs/6wLFsU1q+nihnbvxoYhDJ6bqj6A0Lwr/A0TJnfeQ2mPU/Om7iS9fluSPBLxDdcTHVu51Nc5121ZhifB1VFs4erNtrymZDZIX6IDIXU/C6G51ZEM1dsCD87x2hi6mzGIT5vZn1IvJGuK5CIxDbF+nHztZcKRzTvpijN/PMWhCWbtGbLqaEDtYtAFesPn1aP0YbUI2U9HxdXsOsaxHttecMcwWiJwrzhcOsarZrmftZnHCzFti6A5GnJmYP1pZbR0X9wuHvbOoD52SYID90YkEM0Y9Vu9zPqr7CXY2ww3MbF1MIgRh1Gpz8XV2V1EunLiYTg4jwO/cKQacupiChCMbsNy5NUw7cO1/HfC6RCkRH52mqa79YUC9iBeJwIDEZPdmrFwSiM0wb1r2ki+cFr5ncU5ggedn66NxLhpzherJcS2NsXUxJKTrIu2u5Pk/hON32Y74a3ftMuUq7q8JZbp21AQnMYoIggeigHHm2406s2g2Od067mCZ3fm6/BbG7fcXY1Bw46872rwkXpPbOCG+OJovJa0EUhP4/2MXU2TwI8z14LQj/MVnawjU1rLoiEAFzVw5DF5OIzBORMhFZHWb/50RklfO3SESmefbNFpENIrJZRG6PVRsPG0ScpRd7IQbRGUXjYPhxem3tq/8ZO6vGdGhla7tmAeQMdyyIGGcxge7YSz+Bko9h+DFde4+k5I4L+nWEiLuWgklPBR2orivXHXVHloyxILyB6mhdTHX79XkiydgqPBJyimHgpMDt5vddtVv79iMV9vQwApHocwcakcYgxp4DJ93qWnKZIQQCtJtJtWoLKlzGX0f40tCiHl8XUyxjEI8AszvY/xlwqlJqKvAL4GEAEUkE/gycB0wErhWREEtFWQIoOrJrAcyeJj0fblig19aOxexkg5kR21DhBgyjIWeYU2p6f2wtCNAC0VSjR+zBlWl7C1PHqLHKdfvUH9TX39nKhhlFun6ViSO0tXY8s9eLd65DpC6SjEL4zqduKQv/NTjf0/t3A6JLTET0fl4XU37ofb4IYxBZg+Dsn7sT38zr80cGHmfcTDlDu3YfiDil0ms9Lqbc6N+nm8TMxaSUWigiIzrYv8jz9EPA2GHHApuVUlsBRORJYA6wNkZNPTy4MbZLD/Y5vKOyrgoEOKO8XrAgDMPjJRDpbpA6bwSUrXFdTB3FH0DXgsoa7FoQkc6iBj1KT0zWi/Z014duYm3lm2HyFTAoAhcXuNcnCe0nm2UU6VRUUzU2Z3jHM5+DGTlLV3Y2M84NZq5NV9xLhuT0IBdTjOJ5HdBXsphuAF51/h8K7PTsK8H1fLZDROaKyFIRWbpvX4Trw1oOfbwjwa745b0B1lhbEAVjtP84JUfHIOJBUqrT2VS5a5HXOS6mSFY2zBrsWhD+WdS5kZ3bWBHddZH4K+4mwuk/jPx1iUl6QJGWH1g+HNzS34bjb4avLyJi0vNhzp/bX5vJZOqOQPicdakbq/T/Mai11BlxD1KLyOlogTA1rEPZYyrENr1DqYdx3FMzZ84Me5zlMMNbdK1LFoRXIGJsQSQkaN+1JLTvoHoLs05HY5V2kyQmuxZEcJppKLIH63IQEHmhPv+5M/S5umtBJGdocZh+XceZdKHIKCJk1zLtmkBXVlIyJHUQsI8Uv0BEkOkVjuQMnTmW6ItLgBriLBAiMhX4K3CeUspJp6AE8H6qw4DS3m6bpY+TnOG6LroiEOmFuiJna2PvpAdf8bfYn6MjfOm6XIMpuZ2Wp4PUkcQgQAeqN7+l/4/GxQQeC6K7ApGuZxsHxyYiIbdYz0EJZvwFkccyosHU1wpVrj5SJl0Gb/+vTnDIDutEiSlxEwgRKQaeAz6vlPIWelkCjBWRkcAu4Brgujg00dKXEdFWRDRrQXhJSNABxANbY+9i6gv4UnXJ67YWPRpNy9czyduaO49BgLYgmqq1iyrSQn0G/9yCHsjj72ztjHDM+XP7NS1iSdE4+PwLMOLkTg8Ny6zb9MqHK+fHJYMJYigQIjIfOA0oFJES4E7AB6CUehD4KVAA3C86yt+ilJqplGoRkW8CrwGJwDyl1JoQp7D0d9IdgeiKBQHa/D+wtXcsiHjjS4ca5zYyFsR+Z9GkiGIQnlRXv4spN7Jz95QF0R1iORkyHKNP797rRfSKii0NbiHAXiaWWUzXdrL/q0DI2UJKqVeAV2LRLsthRFq+djN1Jc8cXP9wv7Ag0tyyGkYgdjjB2EgtCNCB6ni5mPojiT648pG4nb6vZDFZLNGTUaBLHnR1voXJMOkPFkRSGv5cj5RsSPeIakQxCEcgqndrF1NCUuTCao6zAnHIEfcsJouly5z6A72mcFfxC0Q/sSAMxoIwdFRmw5Bt6jGVurOoIxXmnkpztfQ6ViAshy4DJ7UvxxANY86EiZd0bY2FQ40AgcgOFIhIYhC+NC0KB7fBgc8idy+BW77CWhCHHFYgLP2X7CFw1aPxbkXv4LWSTBYT6BhOuMJ0wWQPhU8e0//P+n7k5062LqZDFSsQFkt/wFvI0etiSi+M3FU0bKZeze3838GYsyI/tz/N1bqYDjWsQFgs/QFvmYpkT4XRSDKYDBff17VzFx6pLRZv0TzLIYEVCIulP2BiEClZ2mIwgemMKASiq0y4SK8NEsvqvpaYEFGaq4jcKiLZovmbiCwXkXNi3TiLxdJDmMV2TBygKxZEd7DicEgS6TyIryilqoBzgCLgy8BvYtYqi8XSsxgLwqSamiB1JHMgLP2WSAXCyP/5wN+VUisJXXXVYrH0Rfwuphz3+ajTYcRJ8WuTpc8TaQximYgsAEYCd4hIFhCiNKLFYumT+IJcTCLwhRfi1hzLoUGkAnEDMB3YqpSqE5F8tJvJYrEcCgS7mCyWCIjUxXQCsEEpVSEi1wM/Bipj1yyLxdKjmDRXOxfBEgWRCsQDQJ2ITAP+B9gO/CNmrbJYLD1LsIvJYomASAWiRSmlgDnAvUqpe4HeX0HbYrF0DZ8zmzmaGkqWfk+kMYhqEbkD+Dxwiogk4iz+Y7FYDgEyCuCSB2Hs2fFuieUQIlIL4mqgET0fYg8wFPhdzFplsVh6nunXRla51WJxiEggHFF4HMgRkQuBBqWUjUFYLBbLYUykpTauAj4GrgSuAj4SkSti2TCLxWKxxJdIXUw/Ao5RSn1RKfUF4FjgJx29QETmiUiZiKwOs3+8iCwWkUYRuS1o3zYR+VREVojI0gjbaLFYLJYeJFKBSFBKedd2LI/gtY8AszvYfwC4BbgrzP7TlVLTlVIzI2yjxWKxWHqQSLOY/isirwHznedXA6909AKl1EIRGdHB/jKgTEQuiLANFovFYulFIhIIpdT3ReRy4CR0kb6HlVLPx7BdClggIgp4SCn1cLgDRWQuMBeguLg4hk2yWCyW/kXECwYppZ4Fno1hW7ycpJQqFZEBwOsisl4ptTBMux4GHgaYOXOm6qX2WSwWy2FPhwIhItXo0Xy7XYBSSsWksItSqtR5LBOR59FB8ZACYbFYLJbY0KFAKKV6vZyGiGSgg+LVzv/nAD/v7XZYLBZLfydma1KLyHzgNKBQREqAO3HKcyilHhSRQcBSIBtoE5FvAxOBQuB50UsUJgFPKKX+G6t2WiwWiyU0MRMIpdS1nezfAwwLsasKmBaTRlksFoslYiKdB2GxWCyWfoYVCIvFYrGExAoEUN/UGu8mWCwWS58jZjGIQ4W2NsUZv3+HoblpnD9lMBdNG0JRVkrAMet2V5GenMgRBRlxaqXFYrH0Pv3egmhqbePaY4upaWzh5/9Zy0m/eYvvPr2C1bv0ktsvr9rNxX96ny/9fQmtbXYensVi6T+IXkn08GDmzJlq6dKuF3/dXFbNPxZv55llJdQ1tTJ1WA6f7qpkcHYqpZUNPHj90cyePKgHW2yxWCzxRUSWhSuK2u8tCC9jBmTx8zmTWXzHmfz4gglU1jdz5viBLPjuqQzPT+OhhVs4nATVYrFYOqLfxyBCkZPm46unjOKrp4zyb/vqyaO488U1LN1+kGNG5ANQVt1AU0sbw/LS49VUi8ViiRnWgoiQK2cOIy/dx+3PrmLBmj3888PtnP67d7j0/kU0ttgsKIvFcvhhBSJC0pOTuPuq6TS3KuY+towfv7Ca4fnp7Ktu5D8rd8e7eRaLxdLjWBdTFJw+fgCnjC3k1dV7EIHzJw/m3HsWMu+Dz7hsxlCc+lEWi8VyWGAtiChJSkzgomlDuHDqEBIShC+dNII1pVUs2XYw3k2zWCyWHsUKRDe57Khh5KT5uP25VXz57x/z/15c4890amhuZf2eqji30GKxWLqGFYhukpacyK1njqW1TbHjQB2PLNrGa2v2oJTilvmfcN6977Fo8/54N9NisViixk6U60FaWtu44L73qW9u5btnH8m3n1pBmi+R7LQkXr7lFF74ZBcL1u7lrAkDuHzGMAoyUzp/U4vFYokhHU2UswLRw7y3aR+f/9vHJAhMHJLNry6dwhUPLCY5KYGaxhaG56ex80A9yUkJ3HTqaL5+6mjSkhPj2maLxdJ/sTOpe5FTxhZx5vgBiAi/uWwqU4fl8pOLJpKZksQ9V09n4fdPZ8F3ZnHupEHc9+Ymzv7DuyzeUh7vZlssFks7rAURA+qaWth5oJ5xgzpe0vvDreXc8dynbCuv5aZTR3PbOeNITHBTZT8tqeTz8z5i2rBcrj/+CM4cP4CEBJtKa7FYeg5rQfQy6clJnYoDwPGjCvjPt07m6pnDeeCdLTyyaJt/38HaJm765zJ8iQms31PF1/6xlLmPLaO2sSWGLbdYLBYXKxBxJiMliV9fNoXTxhVx94INlFbU09jSyq1PrWBfdSN//cJMPvjBGfzkwom8tX4vVz64mNfW7KG0or7D93197V5W7KzonYuwWCyHJTETCBGZJyJlIrI6zP7xIrJYRBpF5LagfbNFZIOIbBaR22PVxr6CiPCLOZNpVYpvP7WCC+57n4Ub9/GzOZOYNjyXpMQEbjh5JH/70jHsPFjHjY8t48TfvMWjHovDS1lVA994fDnffvITu4aFxWLpMrG0IB4BZnew/wBwC3CXd6OIJAJ/Bs4DJgLXisjEGLWxzzA8P51bzzySjz87QH1TK3//8jFce2xxwDGnjxvAkh+dxfM3n8hxI/O5542NIV1Of3v/M5pa29hWXsdra/b01iVYLJbDjJgJhFJqIVoEwu0vU0otAZqDdh0LbFZKbVVKNQFPAnNi1c6+xNxZo/jLF2ay4DuzOH3cgJDHpPoSOao4jzvOn8DBumb+sXh7wP7Kumb++eF2Lpg6mBEF6Tz4rl3DwmKxdI2+WKxvKLDT87wEOC7cwSIyF5gLUFxcHO6wQ4LEBOHsiQMjOnb68FxOPbKIv7y3lREF6by4spSMlCSaW9uobWrlG6eN4ZOdB/nR86tZvKWcE8cUAnoN7pY2RXKSDT9ZLJaO6Yu9RKg8zrBDYKXUw0qpmUqpmUVFRTFsVt/j1rPGcqC2ia8/vpyl2w/yxrq9/HtFKWeOH8DEIdlcPmMYhZkp/PKVddQ3tVLb2MKl93/ADY8uiXfTLRbLIUBftCBKgOGe58OA0ji1pU8zoziP31w2hew0H2dPHEiCCOt2VzE8X69wl+pL5DeXTeFrjy3ltn+tpLm1jZUllQBs21/LiMKMeDbfYrH0cfqiBbEEGCsiI0UkGbgGeDHObeqzXHNsMedPGYwvMYHEBGHy0Bxy0nz+/WdNHMjts8fz8qe7WbB2LzeeOgoReHGl1VyLxdIxMbMgRGQ+cBpQKCIlwJ2AD0Ap9aCIDAKWAtlAm4h8G5iolKoSkW8CrwGJwDyl1JpYtbM/MHfWKKoamklKSODbZ41lxY4KXlixi2+dMSbiRY5aWtvYV9PI4Jy0GLfWYrH0FWImEEqpazvZvwftPgq17xXglVi0qz8iInz/3PH+53OmD+WHz3/K8h0VLFizh301jfzmsqlhA9cNza18/Z/LeG/Tfl77zixGF2X2VtMtFksc6YsuJkuMOX/KIHyJwnV/+ZCHFm7lueW7uP3ZVSHTYY04vL1hHwr4R5jJeRaL5fCjLwapLTEmNz2Z2ZMH8/6mfdz/uRmsKa3i7tc3cqCuCQHyM1L4+ZxJpCcnctu/VvL2hn386tIpLN1+gGeWlfC9c8eRneoLeM9FW/Zz02PLOGZEPudOGkTJwTq2lddx46mjmDQkJz4XarFYuoUViH7KXVdOBSAlKZEzxg+gqr6Z5z7ZxaDsVBZu2s/OA3UcMzKP/6zazf/MHsd1xxUzZWgOzy3fxb+WlnDDySP971VR18R3n1pJRkoSq0sreXN9GQkCab5E3tu0j/lzj2f8oOx4XarFYukitty3pR0vrSzl20+toLVNcelRQ7n7qmn+YPYVDyxib3UDz9x0IgOzU1FK8Y0nlvP62r08f/NJTBiczca91RTnp1Ne08RVDy2mpa2Nf910IiNtWq3F0uewK8pZoua1NXt4Z8M+7rxoIqk+d8W7tzeUccMjS0hMEE4YXcj63VWUVTfyg9nj+fppo9u9z5Z9NVzxwCIKM1N44RsnkZFijVaLpS9hBcLSo+wor2PeB5/xzoYypgzL5fRxRVwyfWjYxYw+2Lyfz//tI86bPJg/XXcUIsLWfTU89uF2Lpk+lGnDc3v3AiwWix8rEJa48+C7W/jNq+sZmJ3CEfkZLN1+gDYFRx+Rx7NfPzHezbNY+i0dCYS19y29wo2zRpGX7uPDrQfYVFbNV04aSVpyIn98azMrdlYw3bEiWtsUFXVN5KT5SEq0WdgWSzyxAmHpFUSEq48p5upj3Iq7NY0tPPLBNv72/mf86PwJfP3xZazYWYFSMDQ3jVvOHMNlM4bhS0zgQG0Tv3ttAxnJifzw/Al2bW6LpRewAmGJG5kpSVxz7HDmfbCN5dsPUlnfzM2njSYvPZmXVpbyg2c/5X9fXsfxowpYtv0gFXVNtCloaVPcedHEiMuEWCyWrmEFwhJXvnjiCOZ9sI2G5laenHs8k4fqSXU3nDySdzbu47XVe3h/837GDMjkF3Mm8/TSnfzt/c/IS0/m1rPGxrn1FsvhjRUIS1wZlpfO/K8dz9C8NIbmuoUARYTTxw1ot7Lejy+YwMHaJv7wxkaOPiKPk8cWtnvP2sYWdlc2MGaArRllsXQHGwW0xJ1jR+YHiENHiAi/vHQKYwZk8p2nV1Be0xiwv66phev+8iGz71nI+j1VsWiuxdJvsGmulkOStaVVXHL/B+Sl+8hNS2ZAdgrXHFPM85+U8Nb6MjJSkhhdlMmzXz+Rv3/wGU98tINzJw/i8hnDrGVhsXiw8yAshyWvrdnDs8tKSBBhdWklJQfrAfjFnElkpibxnadWcvQReSzbfpDRRRlsK6+jtU1x5MBM5kwfyk2njibRZkNZ+jl2HoTlsOTcSYM4d9IgQM+feHdjGdUNLcyZPhSlFM8t38V7m/bzxROO4M6LJrG/ppH/rNrNf1fv4XevbWB4fjoXTxsS56uwWPou1oKwHLZU1DXxyc4KTjuyKCAltq1NcfYf3iUlKZGXbzmZA7VNPPzeVpSC1KQESisb2FfdyKlHFnHFzGHtSpuHoqy6gZw0HylJiZ0ea7H0JayLyWIJ4uklO/mfZ1fxty/O5KF3t7Jsx0GSExOob25lYHYKWak+NpfVkJ6cyHXHFjP31FEMyEoNeI+W1jZWl1bx9w8+46WVpYwZkMn9n5vBmAFZcboqiyV6rEBYLEE0trQy6//epqKumcaWNu69Zjpzpg+lrU35Z2l/WlLJ397fyosrS/ElJnDDySO5+fQxrN9dxf3vbOGjreXUNrWSkZzIpTOG8uqne6hvbuWB64/m1COL4nyFFktkxEUgRGQecCFQppSaHGK/APcC5wN1wJeUUsudfduAaqAVaAnX+GCsQFii4eGFW/jVK+v5+mmj+cHs8WGP+2x/Lfe8sZF/ryglIzmR2qZWCjNTOH/KIGYU53H6uAHkpPvYU9nA1Q8vpjAzxRYgtBwyxCtI/QjwJ+AfYfafB4x1/o4DHnAeDacrpfbHsH2Wfs5XThrJ5CE5HDeqoMPjRhZmcO81R/ElZ9b35CHZfOGEEaQlB8YbBuWkcsn0odz31ibKaxopyEzh2WUltCrFlUcPC4iDVNY309jcyoDs1ODTWSx9hpgJhFJqoYiM6OCQOcA/lDZhPhSRXBEZrJTaHas2WSxekhITOHFM+5nY4TiqOI8/Fud1eMxZEwZy75ubeHvDPs4YP4AfPv8pjS1tfFpSyZ0XTSQpMYHSinqufngxVfUt/PsbJzHCrrRn6aPEM811KLDT87zE2bYbUMACEVHAQ0qph8O9iYjMBeYCFBcXhzvMYukVJg/NZmB2Cm+s3UtZdQONLW1cdtRQHvtwO8u2H+SqmcN4dPF2KmqbSUwUvvqPpTx384kRZUpZLL1NPEtthJqhZAIiJymlZqDdUN8QkVnh3kQp9bBSaqZSamZRkQ0MWuKLiHDmhIEs3LSPfy7ezomjC7j76unce8102pTi/720lr1VDTzylWN44HNHs21/LV97dCmlFfXxbrrF0o54WhAlwHDP82FAKYBSyjyWicjzwLHAwl5vocXSBc6aMIAnPtpBXVMrd148CYA504dy8bQhrN1dRUpSor/cx11XTuOO5z7lrLvf5UsnjmB0USaDc1MZnJPGkNxUO6/CElfiKRAvAt8UkSfRwelKpdRuEckAEpRS1c7/5wA/j2M7LZaoOHF0Iam+BAoyUjhrwkD/dhFh0pCcgGMvOWooRx+Rx50vruH+d7YE7MtMSeKGk0fy1VNGkmVdUJY4EDOBEJH5wGlAoYiUAHcCPgCl1IPAK+gU183oNNcvOy8dCDzvZHwkAU8opf4bq3ZaLD1Nqi+RX106haKslIhqPQ3PT2fel46hobmV3ZUN7K6sZ3dFA2+s28u9b27isQ+3c/t54zl/ymD++t5W3t6wj59dPInpw3NZsu0Ajy3ezo8vmBAyI0opxeKt5cwoziPVZ60RS3TYiXIWSx9mVUkFP39pLUu3HyQ5KYGmljZy0nzUN7dy6fShPLO8hNY2xTkTB/LQ549ut8re3a9v5L43N/G544r55aVT4nQVlr6MLdZnsRyiTB2Wy9M3nsCzy0tYtKWc648/gpGFGdz8+DKeWrqT8yYPYsyATP741mZeXb2H86cM9r/20UXbuO/NTQzKTuWJj3dw9THDmTosF9BlQv735XW0KcXUYbmcM2mgzaSytMNaEBbLIUhzaxtrSquYNiyH1jbFJfd/wJ7KRhZ8Zxb5Gcks2rKfz/31I84cP5C7rpzK2X9YyJCcVJ6/+SQSEoQ/vL6Re9/cRJovkfrmVsYOyOSJrx1PUVZKvC/N0st0ZEHYFeUslkMQX2IC04fnIiIkJSbw28unUtXQzE2PLWN/TSO3Pb2SkQUZ3HftdHLTk/nR+RNYWVLJzY8v57nlJfzxrU1cNmMoq392Ln//8jGUHKzn2r98yK9eWces/3ubn7ywOt6XaOkDWAvCYjlM+PeKXdz65AryM5KprG/m2a+fyPThuYAOVt/35mYefHcL9c2tjCzM4KVvnUxmivYyf7S1nC8/soSmljbGDMhk/Z5qfnfFVK6cObyDM1oOB2w1V4uln3Dfm5u4+/WN3HLmWL579pHt9pdVN/DERzu4cOrgdmXJ91Y1kJKUQFaqj+v/+hGf7DzIs18/sV1q7sqdFVTWN3PC6AJ8iR07IbzVcS19EysQFks/QSnF2t1VTBiU3a2Ouay6gfPvfZ/y2kZOGVvEcSPzSU5M4M31e/lw6wEACjOTOX3cAI4oSGfKsFxOGVMYcM6/LNzKQwu38PZtp9l5HH0Ym8VksfQTQk3G6woDslJ56VsnMf+jHTy7fBcLN+4DYGB2Cj++YALF+ek8t3wXb28oY39NEwCjijK45YyxXHLUUPZVN3LPGxupbWrlhRWlfP74I7rdJkvvYy0Ii8XSIUopGlvaaG5tIz05qd3kv7qmFl5fu5eHF25lTWkVd5w3nh0H6nhqyU4G56aSmeLjlVtORkRoa1P8d80eFqzZQ3FBBseOyOfksZFX1LX0PNaCsFgsXUZESPUlhp2JnZ6cxJzpQ7lgymC+/dQKfv3qekTgiyeMYMyATH78wmpW7KwgKSGBHzy7irW7q8hL91FZX0qbgj9eexQXTRsCaNdWUWZKuwl/lvhg01wtFkuPkJSYwD1XT+fCqYMpyEjhljPHMmf6ENKTE7nzxTVc+dAiDtY1cfdV01j647NZ/bNzmTA4m9+8up6G5laeXrKTY3/5Jl/8+xK2l9f633fLvhpm37OQ+R/viOPV9U+si8lisfQ4jS2t/kq0tz+7iieX7OSYEXk8cP3RFGa6k/E+2Kwn9F08bQivrt7NmAFZ7DxQR3NrG988fQyXzhjKNQ9/SMlBXQ7991dO4/Kjh8Xlmg5XrIvJYrH0Kt4y5d87ZxyThuZw9czhJCcFOi1OGlPIWRMG8OLKUkYUpPPk146noaWVn7+0lt87s72TkxL4100ncM8bG/n+MysZnJPqXwmwuqGZdzfu490N+5g9eRBneqrnAtQ3tZKYIO3Oa4kMa0FYLJa4sr28lv99eR0/mD0uYG7G2xvKePCdLXzj9DHMOrKIuqYWTvvdOxx9hLZEGltaOfP371JysJ4EgYyUJBZ8ZxaDc9IAXY7koj++T01jCw9efzSTh3Y/u+twxJbasFgsfZYjCjL4yxdmtpu4d/q4ATx14wnMOlKvFJmenMR5kwfx9oYy6ppaeGtdGSUH6/m/y6fy5vdOo6VV8T/PrMIMev/54XbW76mmprGFyx5YxD8/3E5b2+EzIO4NrEBYLJZDhvOmDKahuY13Nuzj2eUlDMhK4fKjhzGyMIMfnj+e9zbt5/9e20BpRT1/eH0jp4wt5M3vnspxI/P58QuruebhD9myrybel3HIYAXCYrEcMhwzIp/CzGQeW7yddzbs49IZQ/3zMj533BFcMn0ID7yzhVN/9za1Ta385MKJFGSm8OiXj+W3l09hw95qrnn4Qw7U6sl9b63fy0PvbuFwcrX3JFYgLBbLIUNignDupEEs3lpOS5viihluRlNCgnDPNUfx1NzjOW5kAbecMZYjB2b59119TDHzv3Y8lXXN3PHcKhZt3s+Njy3j16+u5543NtHY0spP/72aOX/+gPKaRgA+2XGQu17bQGs/dU3ZLCaLxXJIcf6UwTz+0Q6mDsth7MCsdvuPG1XAcaMKQr524pBsvnfOkfz61fW8vX4fIwszmDg4m3vf3MQLK3axvbwOX6Jw0z+X8YPZ4/nS35dQ09jC5KHZzJ48OOR7Hs5YC8JisRxSHDcyn+NH5XPjrNFdev1XTxnFiaMLKMhM5pEvH8vvrpzG6eOKOFDTxEOfP5o/XD2dJdsOcuVDi8lN9zEsL42HFm5FKcWCNXuYfc/CfhPHsGmuFoul39HS2kZLm/KXD2lrU9Q3t5LhrI/x8MItPLtsF3/5wkze3VjGT/69ht9fOY3/9+IaqhtbGD8oi+dvPom05NDlRw4l4pLmKiLzRKRMREIuTSWa+0Rks4isEpEZnn2zRWSDs+/2WLXRYrH0T5ISEwJqSyUkiF8cAObOGs1r35lFcUE6Vxw9nLx0H9/710pE4FeX6mD3D5//lJU7K9h5oK7H2tXWppj/8Q4+3FreY+/ZHWIZg3gE+BPwjzD7zwPGOn/HAQ8Ax4lIIvBn4GygBFgiIi8qpdbGsK0Wi8USkrTkRL504kj+8MZG/u+KacyePIg9lfXc99Zmnv9kFwAXTB3MTy+cyMDs1C6fp7Kume8+vYI315eRlZLEy7ecQnFBek9dRpeIqYtJREYA/1FKTQ6x7yHgHaXUfOf5BuA0YATw/5RS5zrb7wBQSv26s/NZF5PFYokFbW2Kz8prGV2UCegS6Ct2VnCgtomVOyt4cOFWBCjMTCHFl8CowkxGF2Wwfk81q3dV8stLJ4cNcu+vaeSJj3bwj8Xbqaxv4ltnjOUv721ldFEm/7rpBP+qfct3HMSXkMCUYT07I7yv1mIaCuz0PC9xtoXafly4NxGRucBcgOLi4p5vpcVi6fckJIhfHECXQD+qOA+AMycM5LIZw3hk0TaqG1qoa2phU1kNb28oY1RhBqm+RP7fi2uZdWQR6cmBXe6rn+7mf55ZRXVjC6eNK+I7Zx3JtOG5jC7K5BtPLOf6v37ESWMKWbLtAO9t2k9yYgIPXD+jXc2pWBFPgQhV8F11sD0kSqmHgYdBWxA90zSLxWKJnBGFGfy/iycFbDPrcS/ddoArHlzMA+9s4XvnjANgR3kdDy7cwhMf7WDa8Fx+f+XUgFIjF0wdzPYD43hmWQl3v76R/IxkfjB7PK+u3s1N/1zGXVdO4+JpQ2K+bkY8BaIEGO55PgwoBZLDbLdYLJZDBrM+98wR+cyZPoSHFm7lQG0Tm8pqWLLtAAJ8+aQR3HHehJDVZm8+bQw3nzaG6oZmkpMSSElK5LrjivnS3z/m1idX8Oiibdx46mhOGlNIZkpsuvJ4xiAuAL4JnI92Id2nlDpWRJKAjcCZwC5gCXCdUmpNZ+ezMQiLxdIX2V1Zz3n3vkdrm2JkYQanjxvANccO91eejYbm1jb+tbSEP761id2VDSQmCEcX5zF/7vHtloONhLjEIERkPjroXCgiJcCdgA9AKfUg8ApaHDYDdcCXnX0tIvJN4DUgEZgXiThYLBZLX2VwThrLfnw2CUK33UK+xASuO66Yy48eytJtB1m0ZT/lNU1dEofOsBPlLBaLpR9j14OwWCwWS9RYgbBYLBZLSKxAWCwWiyUkViAsFovFEhIrEBaLxWIJiRUIi8VisYTECoTFYrFYQmIFwmKxWCwhOawmyonIPmB7F19eCOzvwebEAtvG7tPX2we2jT2FbWNkHKGUKgq147ASiO4gIkvDzSbsK9g2dp++3j6wbewpbBu7j3UxWSwWiyUkViAsFovFEhIrEC4Px7sBEWDb2H36evvAtrGnsG3sJjYGYbFYLJaQWAvCYrFYLCGxAmGxWCyWkPR7gRCR2SKyQUQ2i8jt8W4PgIgMF5G3RWSdiKwRkVud7fki8rqIbHIe8/pAWxNF5BMR+U9fbKOI5IrIMyKy3vk8T+hLbRSR7zjf8WoRmS8iqX2hfSIyT0TKRGS1Z1vYdonIHc49tEFEzo1T+37nfM+rROR5EcmNV/vCtdGz7zYRUSJSGM82dka/FggRSQT+DJwHTASuFZGJ8W0VAC3A95RSE4DjgW847bodeFMpNRZ403keb24F1nme97U23gv8Vyk1HpiGbmufaKOIDAVuAWY667YnAtf0kfY9AswO2hayXc5v8xpgkvOa+517q7fb9zowWSk1Fb2u/R1xbF+4NiIiw4GzgR2ebfFqY4f0a4EAjgU2K6W2KqWagCeBOXFuE0qp3Uqp5c7/1ehObSi6bY86hz0KXBKXBjqIyDDgAuCvns19po0ikg3MAv4GoJRqUkpV0IfaiF4XPk1EkoB0oJQ+0D6l1ELgQNDmcO2aAzyplGpUSn2GXmf+2N5un1JqgVKqxXn6ITAsXu0L10aHPwD/A3gzhOLSxs7o7wIxFNjpeV7ibOsziMgI4CjgI2CgUmo3aBEBBsSxaQD3oH/obZ5tfamNo4B9wN8dN9hfRSSjr7RRKbULuAs9ktwNVCqlFvSV9oUgXLv64n30FeBV5/8+0z4RuRjYpZRaGbSrz7TRS38XCAmxrc/k/YpIJvAs8G2lVFW82+NFRC4EypRSy+Ldlg5IAmYADyiljgJqib/Ly4/jw58DjASGABkicn18W9Ul+tR9JCI/QrtpHzebQhzW6+0TkXTgR8BPQ+0OsS3ufVF/F4gSYLjn+TC0iR93RMSHFofHlVLPOZv3ishgZ/9goCxe7QNOAi4WkW1o19wZIvJP+lYbS4ASpdRHzvNn0ILRV9p4FvCZUmqfUqoZeA44sQ+1L5hw7eoz95GIfBG4EPiccid59ZX2jUYPBlY6980wYLmIDKLvtDGA/i4QS4CxIjJSRJLRQaIX49wmRETQfvN1Sqm7PbteBL7o/P9F4N+93TaDUuoOpdQwpdQI9Of2llLqevpWG/cAO0VknLPpTGAtfaeNO4DjRSTd+c7PRMeb+kr7ggnXrheBa0QkRURGAmOBj3u7cSIyG/gBcLFSqs6zq0+0Tyn1qVJqgFJqhHPflAAznN9pn2hjO5RS/foPOB+d8bAF+FG82+O06WS0ebkKWOH8nQ8UoLNHNjmP+fFuq9Pe04D/OP/3qTYC04Glzmf5ApDXl9oI/AxYD6wGHgNS+kL7gPnouEgzuiO7oaN2oV0nW4ANwHlxat9mtB/f3DMPxqt94doYtH8bUBjPNnb2Z0ttWCwWiyUk/d3FZLFYLJYwWIGwWCwWS0isQFgsFoslJFYgLBaLxRISKxAWi8ViCYkVCIulDyAip5mKuBZLX8EKhMVisVhCYgXCYokCEbleRD4WkRUi8pCzHkaNiPxeRJaLyJsiUuQcO11EPvSsT5DnbB8jIm+IyErnNaOdt88Ud+2Kx53Z1RZL3LACYbFEiIhMAK4GTlJKTQdagc8BGcBypdQM4F3gTucl/wB+oPT6BJ96tj8O/FkpNQ1de2m3s/0o4NvotUlGoetdWSxxIyneDbBYDiHOBI4GljiD+zR0wbo24CnnmH8Cz4lIDpCrlHrX2f4o8C8RyQKGKqWeB1BKNQA47/exUqrEeb4CGAG8H/OrsljCYAXCYokcAR5VSt0RsFHkJ0HHdVS/piO3UaPn/1bs/WmJM9bFZLFEzpvAFSIyAPxrNB+Bvo+ucI65DnhfKVUJHBSRU5ztnwfeVXpdjxIRucR5jxRnnQCLpc9hRygWS4QopdaKyI+BBSKSgK7S+Q30QkSTRGQZUImOU4Auif2gIwBbgS872z8PPCQiP3fe48pevAyLJWJsNVeLpZuISI1SKjPe7bBYehrrYrJYLBZLSKwFYbFYLJaQWAvCYrFYLCGxAmGxWCyWkFiBsFgsFktIrEBYLBaLJSRWICwWi8USkv8Pv8bpXBuQ5MkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_info.history['loss'])\n",
    "plt.plot(model_info.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806b5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(\"./emotion_detector_models/model_146.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "391b0504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4394 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-51eb68c41a1e>:28: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 59   1  11   7   9  18   1]\n",
      " [ 33  36  11   7   3  17   4]\n",
      " [101   4 317  38 117 189  77]\n",
      " [ 22   0  30 502  27  23   7]\n",
      " [ 66   5  80  97 728 191  13]\n",
      " [118   8 144  73 240 600  18]\n",
      " [ 10   0  52  14  13  10 243]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.14      0.56      0.23       106\n",
      "   disgusted       0.67      0.32      0.44       111\n",
      "     fearful       0.49      0.38      0.43       843\n",
      "       happy       0.68      0.82      0.74       611\n",
      "     neutral       0.64      0.62      0.63      1180\n",
      "         sad       0.57      0.50      0.53      1201\n",
      "   surprised       0.67      0.71      0.69       342\n",
      "\n",
      "    accuracy                           0.57      4394\n",
      "   macro avg       0.55      0.56      0.53      4394\n",
      "weighted avg       0.59      0.57      0.57      4394\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "# Found 28709 images belonging to 7 classes.\n",
    "# Found 3589 images belonging to 7 classes.\n",
    "\n",
    "\n",
    "# nb_train_samples = 28273\n",
    "# nb_validation_samples = 3534\n",
    "nb_train_samples = 28709\n",
    "nb_validation_samples = 4394\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        \n",
    "        target_size=(48, 48),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "291ec912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('./emotion_detector_models/model_146.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc761af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4394 images belonging to 7 classes.\n",
      "{0: 'angry', 1: 'disgusted', 2: 'fearful', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprised'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "#         color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95ab5c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1ravi\\Anaconda\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 896ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Prediction happy happy\n",
      "Prediction surprised surprised\n",
      "Prediction sad happy\n",
      "Prediction neutral neutral\n",
      "Prediction fearful fearful\n",
      "Prediction sad sad\n",
      "Prediction angry angry\n",
      "Prediction neutral neutral\n",
      "Prediction disgusted disgusted\n",
      "Prediction sad sad\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    expanded_image=im\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return load_img(final_path, target_size = (img_width, img_height,3),grayscale=False), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = './DataSet/test/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[np.argmax(predictions[i][0])], image, true_labels[i])\n",
    "    print(\"Prediction\", class_labels[np.argmax(predictions[i][0])], true_labels[i])\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdb4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914e2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7695e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c61c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
